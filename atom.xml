<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[yangpeng's Blog]]></title>
  <link href="http://yangpengg.github.com/atom.xml" rel="self"/>
  <link href="http://yangpengg.github.com/"/>
  <updated>2013-10-18T22:06:02+08:00</updated>
  <id>http://yangpengg.github.com/</id>
  <author>
    <name><![CDATA[yp]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Connect to LEGO EV3 under Mac OS X with Bluetooth]]></title>
    <link href="http://yangpengg.github.com/blog/2013/10/13/connect-to-lego-ev3-under-mac-os-x-with-bluetooth/"/>
    <updated>2013-10-13T20:29:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2013/10/13/connect-to-lego-ev3-under-mac-os-x-with-bluetooth</id>
    <content type="html"><![CDATA[<p>最近买了一套 LEGO Mindstorms EV3 45544，本来买的时候想照着<a href="http://www.tiltedtwister.com/tiltedtwister2.html">Tilted Twister</a>做一个解魔方的机器人，没想到买的机器人版本太新，不只building instruction不能照搬，因为<a href="http://bricxcc.sourceforge.net/nbc/">NXC</a>还没有发布支持EV3的版本，连代码也用不了。</p>

<h2 id="ev3nxt">EV3与NXT的不同</h2>

<p>LOGO前两代的机器人叫NXT，我没有查过NXT的资料，但是估计只是在硬件上自己写了个简单的OS，能够解释LEGO Software编译出来的程序。所以在这层关系上LEGO的OS相当于JVM，Software编译出来的程序相当于class文件，源码文件(一种图形语言，被称为G语言，也叫nxt-g)相当于JAVA源码。</p>

<p>EV3与前两代最大的不同在于，EV3底层运行了一个标准的Linux，在Linux之上LEOG自己开发了一个VM，称为lms2012。换句话说，EV3现在在软件上完全与JVM对应。</p>

<p>EV3的firmware是开源的，源码托管在github上：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">git clone https://github.com/mindboards/ev3sources.git</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>关于源码结构，这里有<a href="http://www.chihayafuru.jp/etrobo/?p=2778">一篇文章</a>可以了解个大概，只不过是日文的，不过有google，日文也是小事。</p>

<p>目录结构大致如下：</p>

<ul>
  <li>extra下是Linux内核，lms2012是 LEGO VM 相关。lms2012中的内容：</li>
  <li>open_first是编译脚本；</li>
  <li>d_xxxx是 VM 要用到的内核模块；</li>
  <li>lms2012和c_xxxx是 LEGO VM 的源码和库函数；</li>
  <li>lmssrc好像是EV3显示屏上对应的程序；</li>
</ul>

<p>关于EV3更多的内容，可以在<a href="http://www.mindstorms.rwth-aachen.de/trac/wiki/EV3">这个wiki</a>上找到。</p>

<h2 id="lego-mindstorms">LEGO Mindstorms编程语言</h2>

<h3 id="nxt-g">nxt-g</h3>

<p>LEGO的机器人是给小孩子玩的，所以LEGO官方只支持一种图形编程语言，叫nxg-g(也就是G语言)。这个东西被LEGO称为凡是C和其它语言可以实现的，G语言也可以实现，不过可以想象一下如果用G实现解魔方的算法，是一件多么dan疼的事情。</p>

<h3 id="nxchttpbricxccsourceforgenetnbc"><a href="http://bricxcc.sourceforge.net/nbc">nxc</a></h3>

<p>这应该是玩LEGO机器人用的最多的语言。nxc实现了一个c语法的子集。从编译过程看，nxc把用c写出来的的程序翻译成一种汇编语言，然后交给nbc再翻译成LEGO VM可以识别的字节码。</p>

<p>虽然只是一个c的子集，但是对程序员而言，这实在是太棒了。而且因为nxc直接把程序翻译成VM可以运行的字节码，也就是说可以在不重新刷EV3官方firmware的情况下直接使用。之所以这么说，是为了引出：</p>

<h3 id="lejoshttpwwwlejosorg"><a href="http://www.lejos.org/">lejos</a></h3>

<p>j指JAVA，名字里有os，也就是说，这完全是重新实现的firmware，想用这个东西写程序，只能刷lejos的firmware。</p>

<p>这个东西在EV3下的实现是用自己的java程序取代了LEGO VM(也就是上边提到的lms2012)。因为自己实现VM，而且编程语言用的是JAVA，所以对程序员而言也是方便了很多。</p>

<h3 id="robotc">robotc</h3>

<p>一个商业的闭源玩意儿，不多说。</p>

<h2 id="macev3">Mac下使用蓝牙连接EV3</h2>

<p>好吧，言归正传，我想用EV3搭出一个可以解魔方的机器人。理论上这事可行，因为有人用NXT搭出来过，NXT都可以，更不要说EV3了。关于使用NXT搭解魔方机器人，有详细资料的至少有两个：一个是上边提到的<a href="http://www.tiltedtwister.com/tiltedtwister2.html">Tilted Twister</a>，另一个是<a href="http://www.mindcuber.com/">MindCuber</a>。</p>

<p>既然理论上可行，那现在面临的困难有两个：</p>

<ul>
  <li>硬件部分，让我这种初玩LEGO，又没有任何艺术细胞的人拼出一个，实在是太难了；</li>
  <li>软件部分，到这篇blog写作时间为止，nxc, lejos都没有正式的版本放出来，如我之前所言，用nxt-g写算法是多么的，呵呵…</li>
</ul>

<p>好吧，实际上要搭这样的一个机器人，不会遇到上述两个难题之外的问题。所以我好像什么也没有做…</p>

<p>一个好的消息是，MindCuber正在出EV3的机器人版本，<a href="http://www.mindcuber.com/mindcub3r/mindcub3r.html">MindCub3r</a>。如果偷个懒，我可以等MindCub3r出来直接参考(MindCub3r会用31313拼，而我的EV3是45544，所以硬件部分只能参考，软件部分应该可以完全Copy)。不过作为一个程序员，至少要做点什么吧。</p>

<p>好吧，我决定试着先写出软件部分，毕竟这个对一个程序员而言，还是比较容易的。但是肯定不是用nxt-g这种玩具语言写。</p>

<p>大体的思路是，扫描魔方的代码用nxt-g实现，这个应该很容易。EV3扫描完魔方之后，扫描的数据传到另一个地方，在另一个地方我可以用正常点的语言实现解魔方的算法，然后回传解法给EV3，EV3再按照解法执行。那么按照这个思路，实现方案有以下两个：</p>

<ul>
  <li>如我之前说过，EV3底层运行着一个标准的Linux，这样我可以编译好一个解魔方算法的c程序(如何让EV3运行native programs见<a href="http://robotnav.wordpress.com/ev3/">这里</a>)，让nxg-g扫描完之后，启动这个程序，然后nxg-g取回解法执行。这个方案可以说perfect，虽然编译native程序比较费事，但是实际解魔方时，除了EV3，再加一个sd卡就是所有的硬件了。但是世界总是很不完美的，这个方案的问题在于：到目前为止，我还没有发现如何在nxt-g的程序里调用native程序。毕竟nxt-g程序是运行在LEGO VM里的，它不是shell，可以一条命令就调用外部程序。好吧，在这个接口没有找到之前，这个方案只好放弃了。</li>
  <li>nxt-g扫描完魔方之后，把数据传到一台pc，我可以直接在pc上写算法，然后回传解法。EV3与外界的无线通讯方式有两种：wifi和bluetooth。好吧，nxt-g是没有wifi通信接口的，所以没有的选，只能用bluetooth了。谢天谢地，总算有个还能用。</li>
</ul>

<p>写到这里总算是跟题目有点关系了。</p>

<p>在Linux下，蓝牙的协议栈用的是bluez，编程的接口用的也是标准的socket编程。可以在<a href="http://lirobo.blogspot.com/2012/08/linux-nxt.html">这里</a>找到Linux下连接NXT的源码。本来问题在这里也就解决，但是…</p>

<ul>
  <li>我家里用的是MacBook，不是Linux；</li>
  <li>(这个是之后才发现的)EV3与NXT蓝牙通信协议不一样，而且EV3没有官方的蓝牙协议文档。所以上边提到的程序即使在Mac OS X下实现，也不能正常通信；</li>
</ul>

<p>先解决第一个问题，这个问题要解决两个方面，(1)找到Mac下蓝牙的编程接口，(2)编译一个命令行objc程序。</p>

<p>第一个问题的第一方面，Apple实现了自己蓝牙的协议栈，主要的参考文档包括，<a href="https://developer.apple.com/library/mac/documentation/devicedrivers/Conceptual/Bluetooth/BT_Intro/BT_Intro.html">Introduction to Bluetooth Device Access Guide</a>, <a href="https://developer.apple.com/library/mac/documentation/DeviceDrivers/Reference/IOBluetooth/_index.html">Bluetooth Framework Reference</a>。这个文档很坑的提到，bluetooth同时提供c和objc的接口，但是却没有在文档中写半个c接口的代码。好吧，只能用objc写了。</p>

<p>关于第二方面，实际上是由于没有找到c接口的引起的，因为用objc接口实现通信逻辑，就只能用delegate模型，实现一个事件驱动的程序。</p>

<p>关于第二个问题，直接上代码(代码中涉及的协议在EV3的源码文件 <a href="https://github.com/mindboards/ev3sources/blob/master/lms2012/c_com/source/c_com.h">c_com.h</a>。说到这个h文件，协议的大部分内容是从注释中得到到，但是坑die的是，关于mailbox的注释有一处是错的，直接导致在这里卡了很长时间，不过还好最后搞定了。)：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
<span class="line-number">84</span>
<span class="line-number">85</span>
<span class="line-number">86</span>
<span class="line-number">87</span>
<span class="line-number">88</span>
<span class="line-number">89</span>
<span class="line-number">90</span>
<span class="line-number">91</span>
<span class="line-number">92</span>
<span class="line-number">93</span>
<span class="line-number">94</span>
<span class="line-number">95</span>
<span class="line-number">96</span>
<span class="line-number">97</span>
<span class="line-number">98</span>
<span class="line-number">99</span>
<span class="line-number">100</span>
<span class="line-number">101</span>
<span class="line-number">102</span>
<span class="line-number">103</span>
<span class="line-number">104</span>
<span class="line-number">105</span>
<span class="line-number">106</span>
<span class="line-number">107</span>
<span class="line-number">108</span>
<span class="line-number">109</span>
<span class="line-number">110</span>
<span class="line-number">111</span>
<span class="line-number">112</span>
<span class="line-number">113</span>
<span class="line-number">114</span>
<span class="line-number">115</span>
<span class="line-number">116</span>
<span class="line-number">117</span>
<span class="line-number">118</span>
<span class="line-number">119</span>
<span class="line-number">120</span>
<span class="line-number">121</span>
<span class="line-number">122</span>
<span class="line-number">123</span>
<span class="line-number">124</span>
<span class="line-number">125</span>
<span class="line-number">126</span>
<span class="line-number">127</span>
<span class="line-number">128</span>
<span class="line-number">129</span>
<span class="line-number">130</span>
<span class="line-number">131</span>
<span class="line-number">132</span>
<span class="line-number">133</span>
<span class="line-number">134</span>
<span class="line-number">135</span>
<span class="line-number">136</span>
<span class="line-number">137</span>
<span class="line-number">138</span>
<span class="line-number">139</span>
<span class="line-number">140</span>
<span class="line-number">141</span>
<span class="line-number">142</span>
<span class="line-number">143</span>
<span class="line-number">144</span>
<span class="line-number">145</span>
<span class="line-number">146</span>
<span class="line-number">147</span>
<span class="line-number">148</span>
<span class="line-number">149</span>
<span class="line-number">150</span>
<span class="line-number">151</span>
<span class="line-number">152</span>
<span class="line-number">153</span>
<span class="line-number">154</span>
<span class="line-number">155</span>
<span class="line-number">156</span>
<span class="line-number">157</span>
<span class="line-number">158</span>
<span class="line-number">159</span>
<span class="line-number">160</span>
</pre></td><td class="code"><pre><code class=""><span class="line">//  ev3_mailbox.m
</span><span class="line">
</span><span class="line">#import &lt;Foundation/NSObject.h&gt;
</span><span class="line">#import &lt;IOBluetooth/objc/IOBluetoothDevice.h&gt;
</span><span class="line">#import &lt;IOBluetooth/objc/IOBluetoothDeviceInquiry.h&gt;
</span><span class="line">#import &lt;IOBluetooth/objc/IOBluetoothRFCOMMChannel.h&gt;
</span><span class="line">#import &lt;IOBluetooth/IOBluetoothUtilities.h&gt;
</span><span class="line">
</span><span class="line">#import &lt;stdio.h&gt;
</span><span class="line">
</span><span class="line">
</span><span class="line">#define MAX_MESSAGE_SIZE 64
</span><span class="line">
</span><span class="line">int create_mailbox_send_data(const char *mailbox_name, const char *msg, unsigned char *out_buffer);
</span><span class="line">int parse_mailbox_data(const unsigned char *data, int size, char *mailbox_name, char *msg);
</span><span class="line">
</span><span class="line">void hex_dump(const unsigned char *data, int size);
</span><span class="line">
</span><span class="line">
</span><span class="line">@interface ConnectionHandler : NSObject
</span><span class="line">{
</span><span class="line">
</span><span class="line">}
</span><span class="line">
</span><span class="line">- (void)rfcommChannelOpenComplete:(IOBluetoothRFCOMMChannel*)channel status:(IOReturn)status;
</span><span class="line">- (void)rfcommChannelData:(IOBluetoothRFCOMMChannel*)channel data:(void *)dataPointer length:(size_t)dataLength;
</span><span class="line">
</span><span class="line">@end
</span><span class="line">
</span><span class="line">
</span><span class="line">@implementation ConnectionHandler
</span><span class="line">
</span><span class="line">- (void)rfcommChannelOpenComplete:(IOBluetoothRFCOMMChannel*)channel status:(IOReturn)status
</span><span class="line">{
</span><span class="line">    if(status != kIOReturnSuccess) {
</span><span class="line">        printf("Connection error!\n");
</span><span class="line">        CFRunLoopStop(CFRunLoopGetCurrent());
</span><span class="line">        return;
</span><span class="line">    }
</span><span class="line">    printf("connection complete\n");
</span><span class="line">
</span><span class="line">    char *mailbox_name = "abc";
</span><span class="line">    char *msg = "ping";
</span><span class="line">    // char *msg = "hello, ev3";
</span><span class="line">    unsigned char mailbox_cmd[MAX_MESSAGE_SIZE];
</span><span class="line">    int size = create_mailbox_send_data(mailbox_name, msg, mailbox_cmd);
</span><span class="line">
</span><span class="line">    hex_dump(mailbox_cmd, size);
</span><span class="line">
</span><span class="line">    if([channel writeSync:mailbox_cmd length:size] != kIOReturnSuccess) {
</span><span class="line">        printf("sendmsg failed\n");
</span><span class="line">        [channel closeChannel];
</span><span class="line">        CFRunLoopStop(CFRunLoopGetCurrent());
</span><span class="line">        return;
</span><span class="line">    }
</span><span class="line">
</span><span class="line">    // CFRunLoopStop(CFRunLoopGetCurrent());
</span><span class="line">}
</span><span class="line">
</span><span class="line">- (void)rfcommChannelData:(IOBluetoothRFCOMMChannel*)channel data:(void *)dataPointer length:(size_t)dataLength
</span><span class="line">{
</span><span class="line">    printf("data received length: %zd\n", dataLength);
</span><span class="line">
</span><span class="line">    hex_dump((unsigned char *)dataPointer, dataLength);
</span><span class="line">
</span><span class="line">    char mailbox_name[MAX_MESSAGE_SIZE];
</span><span class="line">    char msg[MAX_MESSAGE_SIZE];
</span><span class="line">
</span><span class="line">    parse_mailbox_data((unsigned char *)dataPointer, dataLength, mailbox_name, msg);
</span><span class="line">    printf("mailbox name: %s\nmsg: %s\n", mailbox_name, msg);
</span><span class="line">
</span><span class="line">    [channel closeChannel];
</span><span class="line">    CFRunLoopStop(CFRunLoopGetCurrent());
</span><span class="line">}
</span><span class="line">
</span><span class="line">@end
</span><span class="line">
</span><span class="line">
</span><span class="line">
</span><span class="line">int main( int argc, const char *argv[] )
</span><span class="line">{
</span><span class="line">    @autoreleasepool {
</span><span class="line">        NSString *ev3_addr_str = @"xx:xx:xx:xx:xx:xx";
</span><span class="line">
</span><span class="line">        IOBluetoothDevice *ev3 = [IOBluetoothDevice deviceWithAddressString:ev3_addr_str];
</span><span class="line">
</span><span class="line">        ConnectionHandler *handler = [[ConnectionHandler alloc] init];
</span><span class="line">
</span><span class="line">        IOBluetoothRFCOMMChannel *channel;
</span><span class="line">        [ev3 openRFCOMMChannelAsync:&amp;channel withChannelID:1 delegate: handler];
</span><span class="line">
</span><span class="line">        CFRunLoopRun();
</span><span class="line">    }
</span><span class="line">
</span><span class="line">    return 0;
</span><span class="line">}
</span><span class="line">
</span><span class="line">
</span><span class="line">int create_mailbox_send_data(const char *mailbox_name, const char *msg, unsigned char *out_buffer)
</span><span class="line">{
</span><span class="line">    // message counter
</span><span class="line">    out_buffer[2] = 0x12;
</span><span class="line">    out_buffer[3] = 0x34;
</span><span class="line">
</span><span class="line">    // system command without reply, WRITEMAILBOX
</span><span class="line">    out_buffer[4] = 0x81;
</span><span class="line">    out_buffer[5] = 0x9e;
</span><span class="line">
</span><span class="line">    int cur_p = 6;
</span><span class="line">
</span><span class="line">    // mailbox name
</span><span class="line">    int size = strlen(mailbox_name) + 1;
</span><span class="line">    out_buffer[cur_p++] = size;
</span><span class="line">    memcpy(&amp;out_buffer[cur_p], mailbox_name, size);
</span><span class="line">    cur_p += size;
</span><span class="line">
</span><span class="line">    // payload
</span><span class="line">    size = strlen(msg) + 1;
</span><span class="line">    out_buffer[cur_p++] = size &amp; 0xff;
</span><span class="line">    out_buffer[cur_p++] = (size &amp; 0xff00) &gt;&gt; 8;
</span><span class="line">    memcpy(&amp;out_buffer[cur_p], msg, size);
</span><span class="line">    cur_p += size;
</span><span class="line">
</span><span class="line">    // command size
</span><span class="line">    out_buffer[0] = (cur_p - 2) &amp; 0xff;
</span><span class="line">    out_buffer[1] = ((cur_p - 2) &amp; 0xff00) &gt;&gt; 8;
</span><span class="line">
</span><span class="line">    return cur_p;
</span><span class="line">}
</span><span class="line">
</span><span class="line">
</span><span class="line">int parse_mailbox_data(const unsigned char *data, int length, char *mailbox_name, char *msg)
</span><span class="line">{
</span><span class="line">    assert(data[0] + (data[1] &lt;&lt; 8) + 2 == length);
</span><span class="line">    assert(data[4] == 0x81 &amp;&amp; data[5] == 0x9e);
</span><span class="line">
</span><span class="line">    int cur_p = 6;
</span><span class="line">
</span><span class="line">    int size = data[cur_p++];
</span><span class="line">    memcpy(mailbox_name, &amp;data[cur_p], size);
</span><span class="line">    cur_p += size;
</span><span class="line">
</span><span class="line">    size = data[cur_p] + (data[cur_p+1] &lt;&lt; 8);
</span><span class="line">    cur_p += 2;
</span><span class="line">    memcpy(msg, &amp;data[cur_p], size);
</span><span class="line">    cur_p += size;
</span><span class="line">
</span><span class="line">    assert(cur_p == length);
</span><span class="line">
</span><span class="line">    return size;
</span><span class="line">}
</span><span class="line">
</span><span class="line">
</span><span class="line">void hex_dump(const unsigned char *data, int size)
</span><span class="line">{
</span><span class="line">    for(int i=0; i&lt;size; i++) {
</span><span class="line">        printf("%02x", data[i]);
</span><span class="line">    }
</span><span class="line">    printf("\n");
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>编译如下(编译之前把源码中的 ev3_addr_str 改成EV3的蓝牙地址)：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">clang -o ev3_mailbox ev3_mailbox.m -framework foundation -framework iobluetooth</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>打开LEGO Software写一个简单的程序：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/mailbox.png" alt="Bluetooth Mailbox" /></p>

<p>好了，在EV3上执行这个程序，然后在Mac上执行</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">./ev3_mailbox</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果不出意外，应该可以听到一声大象叫，而且EV3的显示屏上应该会显示：”ping”，然后在pc端会显示：”pong”。</p>

<p>“ping”是由Mac发送到EV3的，”pong”是EV3在收到”ping”之后发送回Mac的。</p>

<p>现在软件的问题解决了，就差拼一个解魔方的机器人出来了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What You Think is Wrong]]></title>
    <link href="http://yangpengg.github.com/blog/2013/07/20/what-you-think-is-wrong/"/>
    <updated>2013-07-20T00:01:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2013/07/20/what-you-think-is-wrong</id>
    <content type="html"><![CDATA[<p>最近看东西的时候，发现两个很有意思。之所以有意思，是因为实际的结论跟初看上去时得到的结论很不一样。</p>

<p>一个来自Udacity的<a href="https://www.udacity.com/course/st101">Introduction to Statistics</a>；一个来自<a href="http://www.amazon.com/books/dp/1461471370">An Introduction to Statistical Learning</a>，著名的ESL的兄弟篇，只是更偏向于应用。</p>

<h2 id="gender-bias">Gender Bias</h2>

<p>如果有Udacity的帐户，可以在<a href="https://www.udacity.com/course/viewer#!/c-st101/l-48759015/e-48686794/m-48676955">这里</a>观看。如果没有的话，youtube上也可以看到：
<a href="https://www.youtube.com/watch?v=CLgVLQAEYw8">1</a>
<a href="https://www.youtube.com/watch?v=f3y_weFskL4">2</a>
<a href="https://www.youtube.com/watch?v=pJrwiukN3Ls">3</a>
<a href="https://www.youtube.com/watch?v=o91iPvtqt78">4</a>
<a href="https://www.youtube.com/watch?v=iKTYAsZLbhc">5</a>
<a href="https://www.youtube.com/watch?v=rDw0TIpwJ-c">6</a>
<a href="https://www.youtube.com/watch?v=-GMhV1twy6Y">7</a>
<a href="https://www.youtube.com/watch?v=GD6cQhkoqS4">8</a>
<a href="https://www.youtube.com/watch?v=DeWp0hnRq4g">9</a>
<a href="https://www.youtube.com/watch?v=JWl8lPGhlbY">10</a>
<a href="https://www.youtube.com/watch?v=55eZrE82TqA">11</a>
<a href="https://www.youtube.com/watch?v=8j5hria6Rc8">12</a>
<a href="https://www.youtube.com/watch?v=YkaVgZ-yFrM">13</a>
<a href="https://www.youtube.com/watch?v=tPSj6_m-0_M">14</a>
<a href="https://www.youtube.com/watch?v=-GMhV1twy6Y">15</a>
<a href="https://www.youtube.com/watch?v=GD6cQhkoqS4">16</a>
<a href="https://www.youtube.com/watch?v=4YY-hmqSz30">17</a>
<a href="https://www.youtube.com/watch?v=dOa4Cl0wM0s">18</a></p>

<p>该问题统计了男生和女生申请两门课程的数据，如下：</p>

<p>男生：申请  接受  接受率</p>

<p>课程A：900  450  50%</p>

<p>课程B：100  10  10%</p>

<p>女生：申请  接受  接受率</p>

<p>课程A：100  80  80%</p>

<p>课程B：900  180  26%</p>

<p>现在的问题是，该学校在接受学生时，有没有性别倾向？</p>

<p>通过以上的数据，第一感觉是学校明显更青睐女生，因为两个课程上女生的接受率都比男生要高。</p>

<p>但是计算一下整体接受率，男生 46%，女生 26%。结论完全相反。</p>

<p>课程的最后一节提到一句话，正好可以放这里：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">I never believe in Statistics. I didn't doctor myself.</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="section">逻辑回归的系数变化</h2>

<p>具体请参考 <em>An Introduction to Statistical Learning</em> 的 4.3.3 和4.3.4节内容。</p>

<p>这两节中使用的数据集是Default，是一个逻辑回归问题。记录了信用卡用户是否延期还款(default)，其中涉及到的主要feature包括帐单金额(balance)，是否是学生(student)，收入(income)。</p>

<p>如果只用balance作为feature训练，则可以得到如下训练参数：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/coef-balance.png" alt="Coefficient balance" /></p>

<p>如果只用student，结果如下：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/coef-student.png" alt="Coefficient student" /></p>

<p>如果同时使用balance, income, student，结果如下：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/coef-multi.png" alt="Coefficient Multiple" /></p>

<p>在第二个图和第三个图中最大的变化在于student的系数发生了根本变化(符号改变)。</p>

<p>在第二个图中student的系数是正数，这符合常情，因为学生没有收入来源，更有可能申请分期还款。第三个图中student的系数却表达了相反的意思，学生反而更不倾向于分期还款。</p>

<p>用更数学的方式表达是，在没有任何信息的情况下，学生更倾向于分期(p(student_default))；但是在知道用户收入和帐单的时候(p(student_default | income, balance))，学生不倾向于分期。</p>

<p>其实我想说的是，在多变量回归的时候，因为因变量间的相互作用，训练出来的系数可能已经跟直觉不一致。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Poisson Distribution]]></title>
    <link href="http://yangpengg.github.com/blog/2013/05/25/poisson-distribution/"/>
    <updated>2013-05-25T22:42:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2013/05/25/poisson-distribution</id>
    <content type="html"><![CDATA[<h2 id="section">泊松分布</h2>

<p>泊松分布是一个离散分布，其 pmf 如下：</p>

<script type="math/tex; mode=display">
Pr(X = k) = \frac{\lambda^{k}e^{-\lambda}}{k!}
</script>

<p>其中参数 $\lambda$ 为分布的均值。</p>

<p>泊松分布描述的是：</p>

<ul>
  <li>
    <p>事件发生次数的均值已知；</p>
  </li>
  <li>
    <p>两次事件是否发生相互独立；</p>
  </li>
</ul>

<p>满足以上条件下，事件发生次数的概率。</p>

<p>比如，在商店里某种商品平均一天被购买 $\lambda$ 次，则该商品在一天中被购买 $k$ 次的概率就满足泊松分布。</p>

<h2 id="section-1">泊松分布的由来</h2>

<p>以下推导主要参考可汗学院的公开课&lt;统计学&gt;，可以在<a href="http://v.163.com/movie/2011/6/G/H/M82IC6GQU_M83JASUGH.html">这里</a>观看相关视频。</p>

<p>大学概率与数理统计的课程讲到泊松分布的时候都是直接给出泊松分布的概率分布函数，很少会讲泊松分布的推导过程，其实泊松分布可以由二项分布推导而来。</p>

<p>为了方便推导，假设对如下过程进行建模：</p>

<p>求某个十字路口，在一小时内通过车的数量的概率。</p>

<p>为了对此建模，首先统计该路口一小时内平均通过的车辆，记为 $\lambda$。假设每小时通过车的数量独立同分布，并且每辆车通过也相互独立。</p>

<p>假如不知道泊松分布，只知道二项分布，则可以用二项分布建模，如下：</p>

<h3 id="section-2">第一次尝试</h3>

<p>把一小时分为60份，按照二项分布，$n = 60$，$E(X) = \lambda$ ，则在一分钟是否有车通过的概率：</p>

<script type="math/tex; mode=display">
p = \frac{\lambda}{60}
</script>

<p>那么一小时内通过 $k$ 辆车的概率为：</p>

<script type="math/tex; mode=display">
P(X = k) = (^{60} _k)p^k(1-p)^{60-k}
</script>

<p>以上的建模方法有一个明显的问题，就是当一分钟通过的车辆不止一辆时，以上方法失效。因为二项分布的基本事件只能表达两种状态，发生和未发生，不能表达发生多少次。</p>

<h3 id="section-3">第二次尝试</h3>

<p>当然可以通过再细分时间间隔的方法解决，比如把一小时分为3600份，则基本事件表示一秒内是否有车通过，概率公式如下：</p>

<script type="math/tex; mode=display">
P(X = k) = (^{3600} _k)p^k(1-p)^{3600-k}
</script>

<p>此时</p>

<script type="math/tex; mode=display">
p = \frac{\lambda}{3600}
</script>

<p>以上的过程依然存在问题，假如一秒内有不止一辆车通过，则以上方法又失效了。</p>

<h3 id="section-4">两次尝试之后</h3>

<p>通过以上两次尝试，可以发现，我们实际需要的是 $n$ 趋于无穷时，$X = k$ 的概率。</p>

<p>当 $n\to\infty$，由二项分布推导泊松分布如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
P(X = k) &= \lim\limits_{n\to\infty}(^n_k)(\frac{\lambda}{n})^{k}(1-\frac{\lambda}{n})^{n-k} \\
&= \lim\limits_{n\to\infty}\frac{n!}{(n-k)!k!}\frac{\lambda^k}{n^k}(1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k} \\
&= \lim\limits_{n\to\infty}\frac{n(n-1)(n-2)\cdots(n-k+1)}{n^k}\frac{\lambda^k}{k!}(1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k} \\
&= \lim\limits_{n\to\infty}\frac{n(n-1)(n-2)\cdots(n-k+1)}{n^k}\times\lim\limits_{n\to\infty}\frac{\lambda^k}{k!}\times\lim\limits_{n\to\infty}(1-\frac{\lambda}{n})^n\times\lim\limits_{n\to\infty}(1-\frac{\lambda}{n})^{-k} \\
&= 1\times\frac{\lambda^k}{k!}\times e^{-\lambda}\times1 \\
&= \frac{\lambda^{k}e^{-\lambda}}{k!}
\end{align*}
 %]]&gt;</script>

<h2 id="section-5">泊松分布的应用</h2>

<p>关于应用，阮一峰的博客有一篇很好的文章：<a href="http://www.ruanyifeng.com/blog/2013/01/poisson_distribution.html">泊松分布与美国枪击案</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Life is short]]></title>
    <link href="http://yangpengg.github.com/blog/2013/01/13/life-is-short/"/>
    <updated>2013-01-13T00:31:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2013/01/13/life-is-short</id>
    <content type="html"><![CDATA[<p><em>Aaron H. Swartz</em> committed suicide on <a href="http://tech.mit.edu/V132/N61/swartz.html">January 11, 2013</a> in New York City. Swartz was 26.</p>

<p>R.I.P, Aaron Swartz.</p>

<p>这是斯人的<a href="http://en.wikipedia.org/wiki/Aaron_Swartz">wikipedia页面</a>。这是 @Fenng 写的一篇<a href="http://dbanotes.net/geek/aaron-swartz_smells-like-teen-spirit.html">文章</a>。</p>

<h2 id="section">斯人主要生平：</h2>

<ol>
  <li>14岁参与制定了RSS 1.0的标准；</li>
  <li><a href="http://www.reddit.com/">Reddit</a>的联合创始人之一；</li>
  <li>web开源框架web.py的作者；</li>
  <li>Markdown的设计者之一；</li>
  <li>2011年7月因为大量下载JSTOR的论文被起诉；</li>
  <li>Stanford大学一年级缀学创业，离开学校的原因是：</li>
</ol>

<blockquote>
  <p>I didn’t find it a very intellectual atmosphere, since most of the other kids seemed profoundly unconcerned with their studies. </p>
</blockquote>

<h2 id="section-1">数字时代罗宾汉</h2>

<p>我开始注意到这个世界上有这么一天才是因为去年的JSTOR事件，关于这件事及我对此事的态度可以看 @ruanyf 的<a href="http://www.ruanyifeng.com/blog/2011/08/copyright_of_academic_papers.html">文章</a>。</p>

<p>今天下午浏览weibo的时候，又看到这个熟悉的名字，但是看到内容的时候心里不舒服了很长时间。我与彼人素不相识，只是佩服他所做所为。</p>

<h2 id="section-2">斯人斯文</h2>

<p>斯人blog地址：<a href="http://www.aaronsw.com/weblog/">http://www.aaronsw.com/weblog/</a>。</p>

<p>斯人很有名的一篇blog叫：<a href="http://www.aaronsw.com/weblog/productivity">HOWTO: Be more productive</a>，中文见这里：<a href="http://www.mifengtd.cn/articles/how-to-be-productive-aaron-swartz.html">如何提高效率</a>。本文标题取自这篇文章中的一句话。</p>

<h2 id="section-3">最后</h2>

<p>感谢 <em>Aaron Swartz</em> 为互联网做出的巨大贡献！</p>

<p>R.I.P</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WAND Operator to Calculate Similarity]]></title>
    <link href="http://yangpengg.github.com/blog/2012/12/29/wand-operator-to-calculate-similarity/"/>
    <updated>2012-12-29T10:44:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/12/29/wand-operator-to-calculate-similarity</id>
    <content type="html"><![CDATA[<p>本文主要内容来自论文 <em>Efficient Query Evaluation using a Two-Level Retrieval Process</em> 。论文全文可以在<a href="http://www.cis.upenn.edu/~jstoy/cis650/papers/WAND.pdf">这里</a>下载。</p>

<p>WAND全称Weak AND或者Weighted AND，主要用于计算文档之间的相似度。</p>

<p>在进入正题之前，先给出一条微博中提到的<a href="http://weibo.com/1751942113/zbGSlsXFw">性能对比</a>，不保证数据的权威性，仅仅提供一个大致参考。</p>

<h1 id="ir">IR中与文档相似度相关的几个概念</h1>

<h2 id="httpenwikipediaorgwikiinvertedindex"><a href="http://en.wikipedia.org/wiki/Inverted_index">倒排索引</a></h2>
<p>在<a href="http://en.wikipedia.org/wiki/Vector_space_model">向量空间模型</a>中一篇文档(document)通常会表示为词(term)的向量，比如这样的三个文档:</p>

<ul>
  <li>$d_1$: I love you.</li>
  <li>$d_2$: I hate you.</li>
  <li>$d_3$: I miss you.</li>
</ul>

<p>表示为： $d_1$ = [1, 1, 0, 0, 1], $d_2$ = [1, 0, 1, 0, 1], $d_3$ = [1, 0, 0, 1, 1]。 向量位置依次表示[I, love, hate, miss, you]，其中1表示在文档中出现，0表示没有出现。</p>

<p>上面是文档表示，其相应的倒排索引可以表示为：</p>

<ul>
  <li>I: [1, 2, 3]</li>
  <li>love: [1]</li>
  <li>hate: [2]</li>
  <li>miss: [3]</li>
  <li>you: [1, 2, 3]</li>
</ul>

<p>当然在索引中除了可以出现文档信息，还可以添加其它信息，比如这个term在此文档中出现的次数。文档会有一个唯一的id号，称为DID，在倒排索引中文档通常是按照DID的增序排列。</p>

<h2 id="posting--posting-list">posting 和 posting list</h2>

<p>在倒排索引中以冒号作为分割可以分为两部分，第一部分称为字典(dictionary)，第二部分称为posting(也称为posting list)。所有的posting合在一起称为postings(或者posting lists)。</p>

<h2 id="tf-idf">tf-idf</h2>

<p>这个概念有点复杂，可以直接看<a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">wikipedia</a>和吴军博士数学之美中的<a href="http://googlechinablog.blogspot.com/2006/06/blog-post_3066.html">相关解释</a>。</p>

<h1 id="section">两段评估</h1>

<p>通常完全评估两篇文档的相似度计算量很大，为了减少计算，可以在完全评估之前引入另一个比较廉价的计算(本文称之为预计算)，并且设置一个threshold，只有当这个廉价的计算结果大于threshold的时候才进行费时的完全计算。</p>

<p>WAND算法正是采用了这种想法，而且WAND算法在预计算时也进行了优化，使得其在性能上有很大的提升。</p>

<h2 id="wand">WAND定义</h2>

<script type="math/tex; mode=display">
WAND(X_1, w_1, X_2, w_2, \cdots, X_n, w_n, \theta) = \sum_{i=1}^nx_iw_i \geq \theta
</script>

<p>其中$X_1$为布尔值，$w_i$为权重，$x_i$称为<a href="http://en.wikipedia.org/wiki/Indicator_function">indicator function</a>，定义为</p>

<script type="math/tex; mode=display">
x_i = \{_{0, \ otherwise}^{1, \ if \ X_i \ is \ true}
</script>

<p>整个WAND操作的结果也是一个布尔值。</p>

<p>有趣的是，我们可以用WAND操作定义AND和OR：</p>

<script type="math/tex; mode=display">
AND(X_1, X_2, \cdots, X_k) \equiv WAND(X_1, 1, X_2, 1, \cdots, X_k, 1, k) \\
OR(X_1, X_2, \cdots, X_k) \equiv WAND(X_1, 1, X_2, 1, \cdots, X_k, 1, 1)
</script>

<h2 id="section-1">评估函数</h2>

<p>这里定义的评估函数是一个累加模型(additive scoring model)，也就是说，两个文档的相似程度是由所有的词作出的贡献的和：</p>

<script type="math/tex; mode=display">
Score(d_a, d_b) = \sum_{t\in d_a\cap d_b}\alpha_t w(t, d_b)
</script>

<p>这里我们固定$d_a$，而把$d_b$作为变量，也就是求所有的文档与$d_a$的相似度。</p>

<p>同时注意到一点，这个公式是不对称的，也就是说$d_a, d_b$的相似度，与
$d_b, d_a$的相似度并不相同。</p>

<p>对于以上的Score函数，论文大致解释如下：</p>

<blockquote>
  <p>For example, for the tf × idf scoring model used by many IR systems, $\alpha_t$ is a function of the number of occurrences of $t$ in the $d_a$, multiplied by the inverse document frequency (idf) of $t$ in the index and $w(t,d_b)$ is a function of the term frequency (tf) of $t$ in $d_b$, divided by the document length $|d_b|$.</p>
</blockquote>

<p>从wikipedia中看到关于tf的定义是不确定的，从上文推断，这里的tf应该指的是raw frequency，也就是一个词t在文档d的出现的次数(tf(t,d))。这里的index应该指所有的文档(corpus)。</p>

<p>如果这样理解，上边一段话用数学公式表示为：</p>

<script type="math/tex; mode=display">
\alpha_t = tf(t, d_a) * idf(t) \\
w(t,d_b) = \frac{tf(t, d_b)}{\|d_b\|}
</script>

<p>上面提到公式不对称，主要就是差在一个文档长度上，如果求$Score(d_a, d_b)$则分母是$|d_b|$，如果求$Score(d_b, d_a)$分母为$|d_a|$。如果定义Score为再除以一个$|d_a|$使得公式对称，这样也没有问题，但是通常情况下我们需要的是与$d_a$最相似的n个文档，相似度计算中是否除以$|d_a|$并不会影响相似文档的排名。</p>

<h2 id="section-2">预计算公式</h2>

<p>对于每个词，可以定义一个上界(upper bound)：</p>

<script type="math/tex; mode=display">
UB_t \geq \alpha_t max(w(t, d_1), w(t, d_2), \cdots)
</script>

<p>由上式以及之前的累加模型假设，文档相似度的上界为：</p>

<script type="math/tex; mode=display">
UB(d_a, d_b) = \sum_{t \in d_a \cap d_b}UB_t
</script>

<p>明显可以得到以下结论：</p>

<script type="math/tex; mode=display">
UB(d_a, d_b) \geq Score(d_a, d_b)
</script>

<p>有了以上的结论，再结合之前定义的WAND操作，可以定义一个计算复杂度比较低的公式：</p>

<script type="math/tex; mode=display">
WAND(X_1, UB_1, X_2, UB_2, \cdots, X_n, UB_n, \theta)
</script>

<p>其中$X_i$是indicator变量，$t_i$为$d_a$中出现的词，当$t_i$出现在$d_b$中时$X_i = 1$，否则$X_i = 0$。</p>

<h2 id="section-3">相似度计算</h2>

<p>定义上边的公式之后，计算两个文档$d_a, d_b$的相似度成为以下两个过程：</p>

<ol>
  <li>取$d_a$所有词$t_i$以及相应的$UB_i$，当$t_i$出现在$d_b$时，取$X_i = 1$，否则取$X_i = 0$，将$X_i, UB_i$代入以上公式求解；</li>
  <li>当1中求解结果为true时，计算$Score(d_a, d_b)$，否则不做任何事情；</li>
</ol>

<h2 id="top-n">求解top-n相似文档</h2>

<p>如果按照之前的提到方法计算$d_a$的n个最相似文档，就要取出所有的文档，每个文档作预计算，比较threshold，然后决定是否在top-n之列。这样计算当然可行，但是还是可以优化的。优化的出发点就是尽量减少预计算，论文中提到的算法如下：</p>

<ol>
  <li>提取出$d_a$中所有的词，以及这些词的倒排索引；</li>
  <li>初始化curDoc=0；</li>
  <li>初始化posting数组，使得posting[t]为词t倒排索引中第一个文档；</li>
</ol>

<p>可以定义一个next函数，用于查找下一个进行完全计算的文档，论文中对next描述如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Function next(θ)
</span><span class="line">  repeat
</span><span class="line">    /* Sort the terms in non decreasing order of DID */
</span><span class="line">    sort(terms, posting)
</span><span class="line">    /* Find pivot term - the first one with accumulated UB ≥ θ */
</span><span class="line">    pTerm ← findPivotTerm(terms, θ)
</span><span class="line">    if (pTerm = null) return (NoMoreDocs)
</span><span class="line">    pivot ← posting[pTerm].DID
</span><span class="line">    if (pivot = lastID) return (NoMoreDocs)
</span><span class="line">    if (pivot ≤ curDoc)
</span><span class="line">      /* pivot has already been considered, advance one of the preceding terms */
</span><span class="line">      aterm ← pickTerm(terms[0..pTerm])
</span><span class="line">      posting[aterm] ← aterm.iterator.next(curDoc+1)
</span><span class="line">    else /* pivot &gt; curDoc */
</span><span class="line">      if (posting[0].DID = pivot)
</span><span class="line">        /* Success, all terms preceding pTerm belong to the pivot */
</span><span class="line">        curDoc ← pivot
</span><span class="line">        return (curDoc, posting)
</span><span class="line">      else
</span><span class="line">        /* not enough mass yet on pivot, advance one of the preceding terms */
</span><span class="line">        aterm ← pickTerm(terms[0..pTerm])
</span><span class="line">        posting[aterm] ← aterm.iterator.next(pivot)
</span><span class="line">  end repeat</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>其中用到了几个函数，解释如下：</p>

<h3 id="atermiteratornextn">aterm.iterator.next(n)</h3>

<p>这个函数返回aterm倒排索引中的DID，这个DID要满足DID &gt;= n。</p>

<h3 id="sortterms-posting">sort(terms, posting)</h3>

<p>sort是把terms按照posting当前指向的DID的非递减排序。比如$d_a$所有词的倒排索引为：</p>

<ul>
  <li>$t_0$: [1, 3, 26]</li>
  <li>$t_1$: [1, 2, 4, 10, 100]</li>
  <li>$t_2$: [2, 3, 6, 34, 56]</li>
  <li>$t_3$: [1, 4, 5, 23, 70, 200]</li>
  <li>$t_4$: [5, 14, 78]</li>
</ul>

<p>当前posting数组为：[2, 2, 0, 3, 0]</p>

<p>根据以上两条信息，可以得到：{$t_0$ : 26, $t_1$ : 4, $t_2$ : 2, $t_3$ : 23, $t_4$ : 5}</p>

<p>则排序后的结果为[$t_2$, $t_1$, $t_4$, $t_3$, $t_0$]</p>

<h3 id="findpivottermterms-">findPivotTerm(terms, θ)</h3>

<p>按照之前得到的排序，返回第一个上界累加和$\geq \theta$的term。</p>

<p>引入以下数据：[$UB_0$, $UB_1$, $UB_2$, $UB_3$, $UB_4$] = [0, 1, 2, 3, 4], $\theta$ = 8</p>

<p>因为(2 + 1 + 4) = 7 &lt; 8 而 (2 + 1 + 4 + 3) = 10 &gt; 8，所以此函数返回$t_3$</p>

<h3 id="picktermterms0pterm">pickTerm(terms[0..pTerm])</h3>

<p>在0到pTerm(不包含pTerm)中选择一个term。</p>

<p>还是用之前的数据，则是在[$t_2$, $t_1$, $t_4$](没有$t_3$)中选择一个term返回。</p>

<p>关于选择策略，当然是以可以跳过最多的文档为指导，论文中选择了idf最大的term。</p>

<p>解释了以上几个函数之后，理解上边的伪码应该没有问题。至于要理解为什么这样不会错过相似度大的文档，就需要自己动一翻脑子。可以参考论文中的解释，不过说起来比较啰嗦，这里就不证明了。</p>

<p>最后要提到的一点是，在sort函数中是没有必要完全排序的，因为每一次循环都只是改变了posting中的一条数据，只要把这个数据排好序就可以了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linear Regression and the Theory]]></title>
    <link href="http://yangpengg.github.com/blog/2012/12/16/linear-regression-and-the-theory/"/>
    <updated>2012-12-16T22:30:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/12/16/linear-regression-and-the-theory</id>
    <content type="html"><![CDATA[<h1 id="section">问题</h1>

<p>简单来说，有一堆实数数据，数据的格式如下：</p>

<script type="math/tex; mode=display">
x_1, x_2, x_3, \cdots, x_n, y
</script>

<p>所有的这些数据称为训练集，其中$x$称为feature，$y$称为target。</p>

<p>现在又来了一个数据：</p>

<script type="math/tex; mode=display">
x_1, x_2, x_3, \cdots, x_n
</script>

<p>现在需要做的是根据这些$x$的值，推测出$y$的值。</p>

<p>对这个问题更详细的描述可以看<a href="http://v.163.com/movie/2008/1/B/O/M6SGF6VB4_M6SGHJ9BO.html">Stanford机器学习公开课</a>中相关描述。</p>

<h1 id="section-1">解决方法</h1>

<h2 id="overdetermined-equations">Overdetermined Equations</h2>

<p>假设$y$是$x$的线性函数（顺便说一句lr中的linear是对于$\theta$而言的，并非针对$x$），表达为公式为：</p>

<script type="math/tex; mode=display">
y = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n
</script>

<p>其中$x_0$为截距(intercept term)，其值恒为1。</p>

<p>最容易想到的方法，可以把所有训练集的数据代入这个公式，得到方程组：</p>

<script type="math/tex; mode=display">
\begin{eqnarray} 
\left\{ 
\begin{array}{lll}
y^{(1)} = \theta_0x_0^{(1)} + \theta_1x_1^{(1)} + \theta_2x_2^{(1)} + \cdots + \theta_nx_n^{(1)} \\
y^{(2)} = \theta_0x_0^{(2)} + \theta_1x_1^{(2)} + \theta_2x_2^{(2)} + \cdots + \theta_nx_n^{(2)} \\
\vdots \\
y^{(m)} = \theta_0x_0^{(m)} + \theta_1x_1^{(m)} + \theta_2x_2^{(m)} + \cdots + \theta_nx_n^{(m)}
\end{array} 
\right. 
\end{eqnarray}
</script>

<p>这个方程组有m个方程，n+1个未知数，实际问题中通常是训练集的个数大于feature个数，也就是说m &gt; n+1，这种情况下的方程组称为<a href="http://en.wikipedia.org/wiki/Overdetermined_system">超定方程组</a>，是不能直接求解的。当然可以像当年欧拉和拉普拉斯最初解决天文计算问题一样(<a href="http://www.52nlp.cn/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%E4%BA%8C">here</a>)，把m个方程组分成n+1组，然后每一组合并成一个方程，得到n+1个方程后再求解。不过问题是怎么分成n+1组，这个很是adhoc的。</p>

<h2 id="cost-function">Cost Function</h2>

<p>机器学习上解决这个问题的方法是定义一个损失函数：</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2
</script>

<p>然后选择适当的$\theta$，使得$J(\theta)$最小。</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>这个最小化的算法在机器学习中称为梯度下降：</p>

<ul>
  <li>随机初始化一组$\theta$值；</li>
  <li>朝着减少cost function的方向，不断更新$\theta$值，直到收敛。更新公式为：</li>
</ul>

<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial \theta_j}
</script>

<p>其中$\alpha$为学习速率(learning rate)。</p>

<h2 id="gradient-descent-1">Gradient Descent推导</h2>

<p>假设训练集中只有一个数据，$\frac{\partial J(\theta)}{\partial \theta_j}$计算如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
\frac{\partial J(\theta)}{\partial \theta_j} &= \frac{\partial{(\frac{1}{2}(h_\theta(x) - y)^2)}}{\partial \theta_j} \\
&= 2 * \frac{1}{2}(h_\theta(x) - y) * \frac{\partial{(h_\theta(x) - y)}}{\partial \theta_j} \\
&= (h_\theta(x) - y) * \frac{\partial(h_\theta(x) - y)}{\partial \theta_j} \\
&= (h_\theta(x) - y) * \frac{\partial(\sum_{i=0}^n\theta_ix_i - y)}{\partial \theta_j} \\
&= (h_\theta(x) - y)x_j
\end{align*}
 %]]&gt;</script>

<p>代入更新公式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
\theta_j &= \theta_j - \alpha(h_\theta(x) - y)x_j \\
&= \theta_j + \alpha(y - h_\theta(x))x_j
\end{align*}
 %]]&gt;</script>

<p>对于有m个数据集的情况可以得到如下公式：</p>

<script type="math/tex; mode=display">
\begin{align*}
\theta_j := \theta_j + \alpha\sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}
\end{align*}
</script>

<h2 id="gradient-descent-2">Gradient Descent直观解释</h2>

<p>$J(\theta)$是一个关于$\theta$的多元函数，高等数学的知识说，$J(\theta)$在点<script type="math/tex">P(\theta_0, \theta_1, \cdots, \theta_n)</script>延梯度方向上升最快。现在要最小化 $J(\theta)$，为了让$J(\theta)$尽快收敛，就在更新$\theta$时减去其在P点的梯度。</p>

<p>在最终推导出的更新公式中，可以得出以下直观结论：如果遇到一个数据使得$(y - h_\theta(x))$比较小，这时候$\theta$的更新也会很小，这也符合直观感觉。当一个数据使得差值比较大时，$\theta$的更新也会比较大。</p>

<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>

<p>以上的讨论的算法叫batch gradient descent，batch指的是，每次更新$\theta$的时候都需要所有的数据集。这个算法有两个缺陷：</p>

<ul>
  <li>数据集很大时，训练过程计算量太大；</li>
  <li>需要得到所有的数据才能开始训练；</li>
</ul>

<p>比如一个场景下，我们训练了一个lr模型，应用于线上环境，当这个模型跑在线上的时候我们会收集更多的数据。但是上面两个问题使得我们不能及时更新模型，而这正是随机梯度下降要解决的问题。</p>

<p>在之前的推导过程中已经给出了sgd的更新公式，只是没有指出，现正式提出sgd的更新公式：</p>

<p>loop for every (x, y) in training set until convergence:</p>

<script type="math/tex; mode=display">
\theta_j := \theta_j + \alpha(y - h_\theta(x))x_j
</script>

<p>与bgd唯一的区别是，无论数据集有多少，每次迭代都只用一个数据。这样当有新的数据时，直接通过上式更新$\theta$，这就是所谓的online learning。又因为每次更新都只用到一个数据，所以可以显著减少计算量。</p>

<h1 id="sgdpython">sgd的Python实现</h1>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#!/usr/bin/env python
</span><span class="line"># -*- coding: utf-8 -*-
</span><span class="line">
</span><span class="line">import re
</span><span class="line">import numpy as np
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">def load_data(filename):                                                
</span><span class="line">    feature = []                                                        
</span><span class="line">    target = []                                                        
</span><span class="line">    f = open(filename, 'rb')                                            
</span><span class="line">    for line in f.readlines():                                          
</span><span class="line">        sample = re.split(' +', line.strip())                          
</span><span class="line">        feature.append([1, ] + sample[0:-1])                             
</span><span class="line">        target.append(sample[-1])                                      
</span><span class="line">                                                                       
</span><span class="line">    return np.array(feature, np.float), np.array(target, np.float)       
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">def sgd(feature, target, iter=200, step=0.001):                       
</span><span class="line">    theta = np.zeros(feature.shape[1])                                  
</span><span class="line">    # theta = np.ones(feature.shape[1])                                
</span><span class="line">                                                                       
</span><span class="line">    for it in range(iter):                                              
</span><span class="line">        for i in range(feature.shape[0]):                             
</span><span class="line">            error = target[i] - sum(theta * feature[i])                
</span><span class="line">            theta = theta + step * error * feature[i]                  
</span><span class="line">                                                                       
</span><span class="line">        predict = [sum(theta * sample) for sample in feature]            
</span><span class="line">        mse = sum((target - predict) ** 2) / feature.shape[0]            
</span><span class="line">        print it, 'mse:', mse
</span><span class="line">                                                                       
</span><span class="line">    return theta                                                        
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">def normalize(feature):                                                
</span><span class="line">    mu = feature.mean(0)                                                
</span><span class="line">    std = feature.std(0)                                                
</span><span class="line">                                                                       
</span><span class="line">    for j in range(1, feature.shape[1]):                                
</span><span class="line">        feature[:, j] = (feature[:, j] - mu[j]) / std[j]                 
</span><span class="line">                                                                       
</span><span class="line">    return feature, mu, std                                            
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">if __name__ == '__main__':                                              
</span><span class="line">    datafile = 'housing_data'
</span><span class="line">    x, y = load_data(datafile)
</span><span class="line">    x, mu, std = normalize(x)
</span><span class="line">    theta = sgd(x, y)
</span><span class="line">    print theta</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>代码中使用的数据集可以从<a href="http://amachinelearningtutorial.googlecode.com/files/housing_data">这里</a>下载，描述在<a href="http://amachinelearningtutorial.googlecode.com/files/housing_description">这里</a>。</p>

<p>代码中normalize函数用于对feature进行归一化处理，可以尝试一下去掉normalize过程，对于这个数据集会得出很出乎意料的结果。</p>

<h1 id="section-2">概率解释</h1>

<p>在以上的讨论中，得出$y$与$x$的关系是线性假设，使用梯度下降也可以从高数中得到依据，唯有损失函数好像是拍脑袋想出来的。有那么多的函数可以用，为什么单选择了一个二次式做为损失函数。其实这里选择二次函数是有其理论基础的。</p>

<p>$y$与$x$满足以下公式：</p>

<script type="math/tex; mode=display">
y^{(i)} = \theta^{T}x^{(i)} + \varepsilon^{(i)}
</script>

<p>其中$\varepsilon^{(i)}$称为误差，可能由两个原因产生：</p>

<ul>
  <li>feature选择的不合适；</li>
  <li>随机噪声；</li>
</ul>

<p>又假设$\varepsilon^{(i)}$独立同分布，且满足均值为0，方差为$\sigma^2$的高斯分布，即：</p>

<script type="math/tex; mode=display">
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\varepsilon^{(i)})^2}{2\sigma^2}}
</script>

<p>也就是：</p>

<script type="math/tex; mode=display">
p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}}
</script>

<p>以上是一个关于$y$, $X$的公式，可以定义一个似然函数，形式如同上式，但是似然函数是关于$\theta$的公式：</p>

<script type="math/tex; mode=display">
L(\theta) = L(\theta; X,y) = p(y|X; \theta)
</script>

<p>根据之前$\varepsilon^{(i)}$的独立性假设，$L(\theta)$可以记做</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
L(\theta) &= \prod_{i=1}^m{p(y^{(i)} | x^{(i)}; \theta)} \\
&= \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}}
\end{align*}
 %]]&gt;</script>

<p>现在已经观察到了很多数据($x$, $y$)，那么什么样的模型才能让这些数据出现的可能性最大。这就是最大似然估计的出发点，也就是求解$\theta$以最大化这些数据出现的概率，即最大化似然函数$L(\theta)$。</p>

<p>关于最大似然估计方法更多解释可以看<a href="http://www.cchere.com/article/1522559">这里</a>。</p>

<p>当然更多时候最大化的是$logL(\theta)$，而不是直接最大化$L(\theta)$，因为log函数单调递增函数，所以这个转化不会影响$\theta$的最终取值。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
l(\theta) &= logL(\theta) \\
&= log\prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}} \\
&= \sum_{i=1}^m log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}} \\
&= mlog\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
\end{align*}
 %]]&gt;</script>

<p>因此最大化$l(\theta)$也就是最小化：</p>

<script type="math/tex; mode=display">
\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
</script>

<p>也就是之前出现的$J(\theta)$。</p>

<p>至此，我们从概率和最大似然估计的角度解释了$J(\theta)$选择这个二次式是合理的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MathJax Test]]></title>
    <link href="http://yangpengg.github.com/blog/2012/12/15/mathjax-test/"/>
    <updated>2012-12-15T23:12:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/12/15/mathjax-test</id>
    <content type="html"><![CDATA[<h1 id="instruction">Instruction</h1>

<p><a href="http://www.idryman.org/blog/2012/03/10/writing-math-equations-on-octopress/">Here</a> is a good instructions to set the MathJax for Octopress.</p>

<p>There are two things to point:</p>

<ul>
  <li>‘gem install kramdown’ is not needed, cause of the installed when setup Octopress;</li>
  <li>I have not found something useful about ‘change Gemfile to kramdown’;</li>
</ul>

<h1 id="latex-test">Latex Test</h1>

<h2 id="gaussian-function">Gaussian Function</h2>

<script type="math/tex; mode=display">
f(x; \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
</script>

<h2 id="equation-used-in-gradient-descent">Equation Used in Gradient Descent</h2>

<script type="math/tex; mode=display">
h_\theta(x) = \sum_{i=0}^n\theta_ix_i = \Theta^Tx
</script>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^i) - y^i)^2
</script>

<script type="math/tex; mode=display">
\theta_j := \theta_j + \alpha\sum_{i=1}^m(y^i - h_\theta(x^i))x_j^i
</script>

<h2 id="a-long-equation">A Long Equation</h2>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
 %]]&gt;</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Decode and Encode in Python]]></title>
    <link href="http://yangpengg.github.com/blog/2012/12/13/decode-and-encode-in-python/"/>
    <updated>2012-12-13T15:09:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/12/13/decode-and-encode-in-python</id>
    <content type="html"><![CDATA[<p>本文主要针对2.7.x版本，对3.x版本可能不适用。</p>

<h1 id="decode-encode">什么是Decode, Encode</h1>

<p>Python内部使用Unicode对所有的字符编码。对于Python而言，Decode指的是将其它编码转化为Unicode，Encode指把Unicode转化为其它编码。</p>

<p>本文只针对utf-8与Unicode的转化，gbk, gb2312就不添乱了。不过可以简单说一句，Python识别源文件的编码主要根据两个内容：</p>

<ul>
  <li>源码中的coding，如：</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line"># -*- coding: utf-8 -*-</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>源码文件的实际编码</li>
</ul>

<h1 id="decode-encode-unicode">decode, encode, unicode函数</h1>

<p>在Python console输入以下内容：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ustring = u'我们'
</span><span class="line">ustring                       # u'\u6211\u4eec'
</span><span class="line">ustring.encode('utf-8')       # '\xe6\x88\x91\xe4\xbb\xac'
</span><span class="line">
</span><span class="line">string = '我们'
</span><span class="line">string                        # '\xe6\x88\x91\xe4\xbb\xac'
</span><span class="line">string.decode('utf-8')        # u'\u6211\u4eec'
</span><span class="line">unicode(string, 'utf-8')      # u'\u6211\u4eec'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>以上的内容是Mac下的结果，在Linux应该也没有问题，但不保证在Windows下也会产生相同的结果。原因还是以上两条，而Mac和Linux的shell编码是utf-8的，Windows就不清楚了。</p>

<p>从以上的代码可以得出：</p>

<ul>
  <li>
    <p>unicode和decode的作用是一样的，都是把其它编码转化成Unicode；</p>
  </li>
  <li>
    <p>字符前加’u’表示Unicode编码，不加任何东西则是默认编码；</p>
  </li>
  <li>
    <p>encode和decode确实是按照之前提到的那样工作；</p>
  </li>
</ul>

<h1 id="section">读写文件时的编码转化</h1>

<p>继续输入以下代码：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">open('unicode.txt', 'wb').write(ustring)                   # UnicodeDecodeError
</span><span class="line">open('utf-8.txt', 'wb').write(ustring.encode('utf-8'))
</span><span class="line">open('utf-8.txt', 'rb').read()                             # '\xe6\x88\x91\xe4\xbb\xac'
</span><span class="line">open('utf-8.txt', 'rb').read().decode('utf-8')             # u'\u6211\u4eec'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>第一条语句是报错的，也就是说Python的内部编码字符串是不能直接写到文件的，应该先encode到一种编码之后再写入到文件。</p>

<p>read调用读取的是原始内容，不经过任何编码，所以需要在程序中自己处理编码问题。比如要把gbk转化成utf-8，则这样调用：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ustring = open('gbk.txt', 'rb').read().decode('gbk')
</span><span class="line">open('utf-8.txt', 'wb').write(ustring.encode('utf-8'))</span></code></pre></td></tr></table></div></figure></notextile></div>

<h1 id="json">读写json时的编码转化</h1>

<h2 id="jsondump">json.dump</h2>

<p>输入以下代码：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">import json
</span><span class="line">
</span><span class="line">address = [{'name': '杨xx', 'country': '中国'}, {'name': '卢xx', 'country': '美国'}]
</span><span class="line">json.dump(address, open('address.json', 'wb'), indent=2)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>然后打开address.json文件，看到的内容如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">[
</span><span class="line">  {
</span><span class="line">    "country": "\u4e2d\u56fd", 
</span><span class="line">    "name": "\u6768xx"
</span><span class="line">  }, 
</span><span class="line">  {
</span><span class="line">    "country": "\u7f8e\u56fd", 
</span><span class="line">    "name": "\u5362xx"
</span><span class="line">  }
</span><span class="line">]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>之所以看到这样的结果，是因为json模块内部对address转化成字串的时候，把字串从其它编码解码为Unicode（其实json.dump还有一个参数encoding，默认为’utf-8’，这里没有给出），然后又对Unicode进行了转义（这里write没有报UnicodeDecodeError，是因为json内部把Unicode二进制转化成了’\u4e2d’这样的串，这就是所谓的转义），最后write转义后的字符。</p>

<p>如果这种情况下想要等到正常的中文串，可以这样做：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">json.dump(address, open('address2.json', 'wb'), indent=2, ensure_ascii=False)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>设置ensure_ascii=False禁止json进行解码和转义，又因为address的编码就是’utf-8’的，所以write的时候不会报错。</p>

<p>但是如果遇到这种情况：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">address = [{u'name': u'杨xx', u'country': u'中国'},
</span><span class="line">           {u'name': u'卢xx', u'country': u'美国'}]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>直接调用：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">json.dump(address, open('address.json', 'wb'), indent=2)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>没有悬念，会得到如上address.json的结果。需要指出的是这次json在内部判断出字符是Unicode编码，所以省略了解编的步骤，只进行转义。</p>

<p>如果想得到中文，按照以上的经验应该输入:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">json.dump(address, open('address2.json', 'wb'), indent=2, ensure_ascii=False)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>好吧，出错了，又是UnicodeEncodeError。简单分析一下可以得到结论，ensure_ascii=False保证不会进行解码（当然这里不需要解码）和转义，这样json输出给write的就直接是Unicode字符，之前已经提到过了，write是不能直接写Unicode的。</p>

<p>那么如何让json输出正常的中文呢？一种比较容易想到的就是把整个address的字符全部用utf-8编码之后再像写address2.json那样输出。但是这种方法代价比较大，另一种比较好的方式是：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">jsonunicodestr = json.dumps(address, indent=2, ensure_ascii=False)
</span><span class="line">open('address2.json', 'wb').write(jsonunicodestr.encode('utf-8'))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>第一句把address转化成一个字符串，而且不进行转义，所以整个jsonunicodestr是Unicode字串。然后把字串编码成utf-8写入文件。</p>

<p>当然也可以合二为一：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">import codecs
</span><span class="line">
</span><span class="line">json.dump(address, codecs.open('address2.json', 'wb', 'utf-8'), indent=2, ensure_ascii=False)</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="jsonload">json.load</h2>

<p>无论是从address.json还是address2.json，调用json.load的时候得到的都是Unicode编码的字符。</p>

<h1 id="python">关于Python编码处理几点想法</h1>
<ul>
  <li>任何时候使用utf-8；</li>
  <li>怎么进，怎么出，尽量不涉及编码问题；</li>
  <li>json模块是个意外，经过json的所有字符都会解码成Unicode；</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[View Transition follow Finger]]></title>
    <link href="http://yangpengg.github.com/blog/2012/11/02/view-transition-follow-finger/"/>
    <updated>2012-11-02T11:11:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/11/02/view-transition-follow-finger</id>
    <content type="html"><![CDATA[<p>I use <a href="http://getprismatic.com/">Prismatic</a> as my main reading tool on iphone. I find the view transition of the app is very intresting(you can have a try <a href="https://itunes.apple.com/us/app/prismatic-always-interesting/id551206444?mt=8">here</a>), that i want to know how it does. By google, i find a <a href="http://iappexperience.com/post/23551184719/chromeless">blog</a> and you can get the <a href="https://github.com/dyang/DYNavigationController">source code</a>. With the demo you can swipe the ui to transition, but can’t let the transition follows your finger. So i decide to write a demo myself.</p>

<p>I put my code on <a href="https://github.com/hermitinhistory/FollowFinger">github</a>, and you can get the most explanation from the blog above.</p>

<p>Finally, big thanks to the author of the blog.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Minhash and LSH]]></title>
    <link href="http://yangpengg.github.com/blog/2012/09/05/minhash-and-lsh/"/>
    <updated>2012-09-05T09:05:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/09/05/minhash-and-lsh</id>
    <content type="html"><![CDATA[<p>本文主要参考<a href="http://i.stanford.edu/~ullman/mmds.html">Mining of Massive Datasets</a>中第三章相关内容。</p>

<h1 id="jaccard-similarity">Jaccard Similarity</h1>

<p>在推荐系统经常要做相似度计算，其中比较简单的一种是<a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard Similarity</a>。Jaccard Similarity描述如下：<br />
设A,B为两个特征向量，比如为两个user投过票的电影，则  </p>

<script type="math/tex; mode=display">
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
</script>

<p>J(A,B)的值介于0-1之间，数值越大，相似度越高。</p>

<p>Jaccard相似度计算很简单，但是当特征向量维数很高的时候(比如文档的特征向量经常会上万维)，这个计算也会很耗时，这时通用的方法是降维，Minhash也可以算作针对Jaccard相似度的降维方法。</p>

<h1 id="minhash">Minhash</h1>

<p>Minhash思想是用一个维度比较小的<em>signatures</em>代替原来维度很大的特征向量，而且可以通过计算这个signatures向量Jaccard相似度去估计原来特征向量的Jaccard相似度。</p>

<h2 id="section">特征向量的矩阵表示</h2>

<p>如S1 = {a, d}, S2 = {c}, S3 = {b, d, e}, S4 = {a, c, d}，则表示为矩阵：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/matrix-representation.png" alt="Matrix Representation" /></p>

<p>以电商为例，这个矩阵可以解释为：用户S1购买过a,d两个商品，S2购买过c，S3购买过b,d,e，…</p>

<h2 id="minhash-1">Minhash计算原理</h2>

<p>为了推荐物品给S1，思路可以是计算出和S1相似的用户，查看这个(些)用户的购买记录，推荐其中S1没有购买过的商品。可以使用Jaccard相似度计算S1的相似用户，但是现在用户的特征向量是5维的，是不是可以使用更少的维度就可以得到Jaccard相似度。</p>

<p>取Element的一个全排列，以beadc为例，这个排列定义了一个hash函数，这个函数的矩阵可以表示：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/permutation.png" alt="a permutation" /></p>

<p>根据这个hash函数，每个特征向量的minhash值定义为：第一个value不为0的Element的值。</p>

<p>从矩阵中可以的得到：<br />
h(S1) = a, h(S2) = c, h(S3) = b, h(S4) = a</p>

<p>可以取另一个Element的不同的全排列，再进行一次以上的步骤，可以得到一组不同的minhash值。将所有的minhash值组合成矩阵，则新生成的矩阵可以作为新的特征向量。如果只取两个排列的话，则原来的5维特征向量降到了2维。</p>

<h2 id="minhash-2">计算Minhash</h2>

<p>实际应用中，如果特征维度很高的话，产生一次全排列是很费时的，所以通常并不会通过产生全排列计算minhash值。</p>

<p>如果特征向量为k维，则可以取n个hash函数，将0,1,…,k-1重新映射到0,1,…,k-1。类比于全排列，这些hash函数的意义相当于将第r维的特征放到了h(r)维，相当于产生了n个全排列。当然hash函数不可避免会有冲突，但是只要k足够大(如果不够大的话，就不需要minhash了)，而冲突不太多的时候，这个影响可以忽略。</p>

<p>如果以h1,h2,…,hn表示n个随机hash函数，r表示原矩阵的行数，SIG(i, c)表示为新生成矩阵第i个hash函数(也就是第i行)，第c列的元素。首先设所有SIG中的元素为MAXINT，然后对每一行r作如想处理：</p>

<p><img src="http://yangpengg.github.com/images/blogpng/algo-minhash.png" alt="algo minhash" /></p>

<p>Minhash的python实现：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#!/usr/bin/env python
</span><span class="line"># -*- coding: utf-8 -*-
</span><span class="line">
</span><span class="line">
</span><span class="line">def minhash(data, hashfuncs):
</span><span class="line">    '''
</span><span class="line">        see mining-of-massive-datasets.pdf ch3 minhash for detail
</span><span class="line">    '''
</span><span class="line">
</span><span class="line">    # DEBUG = True
</span><span class="line">    DEBUG = False
</span><span class="line">
</span><span class="line">    rows, cols, sigrows = len(data), len(data[0]), len(hashfuncs)
</span><span class="line">
</span><span class="line">    # fucking the shadow copy
</span><span class="line">    # sigmatrix = [[1000000] * cols] * sigrows
</span><span class="line">    sigmatrix = []
</span><span class="line">    for i in range(sigrows):
</span><span class="line">        sigmatrix.append([10000000] * cols)
</span><span class="line">
</span><span class="line">    for r in range(rows):
</span><span class="line">        hashvalue = map(lambda x: x(r), hashfuncs)
</span><span class="line">        if DEBUG: print hashvalue
</span><span class="line">        for c in range(cols):
</span><span class="line">            if DEBUG: print '-' * 2, r, c
</span><span class="line">            if data[r][c] == 0:
</span><span class="line">                continue
</span><span class="line">            for i in range(sigrows):
</span><span class="line">                if DEBUG: print '-' * 4, i, sigmatrix[i][c], hashvalue[i]
</span><span class="line">                if sigmatrix[i][c] &gt; hashvalue[i]:
</span><span class="line">                    sigmatrix[i][c] = hashvalue[i]
</span><span class="line">                if DEBUG: print '-' * 4, sigmatrix
</span><span class="line">
</span><span class="line">        if DEBUG:
</span><span class="line">            for xxxxxxx in sigmatrix:
</span><span class="line">                print xxxxxxx
</span><span class="line">            print '=' * 30
</span><span class="line">
</span><span class="line">    return sigmatrix
</span><span class="line">
</span><span class="line">
</span><span class="line">if __name__ == '__main__':
</span><span class="line">    def hash1(x):
</span><span class="line">        return (x + 1) % 5
</span><span class="line">
</span><span class="line">    def hash2(x):
</span><span class="line">        return (3 * x + 1) % 5
</span><span class="line">
</span><span class="line">    data = [[1, 0, 0, 1],
</span><span class="line">            [0, 0, 1, 0],
</span><span class="line">            [0, 1, 0, 1],
</span><span class="line">            [1, 0, 1, 1],
</span><span class="line">            [0, 0, 1, 0]]
</span><span class="line">
</span><span class="line">    print minhash(data, [hash1, hash2])</span></code></pre></td></tr></table></div></figure></notextile></div>

<h1 id="locality-sensitive-hashing">Locality-Sensitive Hashing</h1>

<p>在推荐系统中，通常需要的是与S1 n个最相似的，而不是所有的。<a href="http://en.wikipedia.org/wiki/Locality_sensitive_hashing">Locality-Sensitive Hashing</a>的提出就是为了过滤明显不相似的，以此来减少计算。</p>

<p>假设向量的维度为n，将n维分为b份，称为bands，每份为n/b维。同时存在b个hash函数，每个bands对应一个hash函数，每个hash函数接受n/b个参数，产生一个数值，把相同数值的元素分为一组。</p>

<p>对每个binds中的n/b维特征向量进行hash计算，比如对于{S1, S2, S3, S4}，3个binds而言，第一个binds可能产生的数据为{S1, S2}, {S3}, {S4}，第二个binds为{S1}, {S2, S4}, {S3}，第三个binds为{S1, S3}, {S2, S4}。则为了计算得到和S1最相似的n个，则只需要计算J(S1, S2), J(S1, S3)，这样达到了减少了计算的目的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Octopress Setup]]></title>
    <link href="http://yangpengg.github.com/blog/2012/08/21/octopress-setup/"/>
    <updated>2012-08-21T16:02:00+08:00</updated>
    <id>http://yangpengg.github.com/blog/2012/08/21/octopress-setup</id>
    <content type="html"><![CDATA[<p>本来没有写过blog，4月份弄了个vps，顺便在上边搭了个wordpress，原打算是要写些技术类的文章，但是一直也没有这个习惯，所以到现在那上边也只有一篇。现在想把blog放到github上，正好也改用Octopress。</p>

<p>Octopress部署起来也没有什么麻烦，只是有一点要注意，想在github上搭User Page，Repository name必须是username.github.com，比如我的用户名是yangpengg，repository就只能是yangpengg.github.com，其它的都不能正常部署。</p>

<p>主要参考以下内容：</p>

<ul>
  <li>
    <p><a href="http://octopress.org/docs/">Octopress Documentation</a></p>
  </li>
  <li>
    <p><a href="http://code.dblock.org/octopress-setting-up-a-blog-and-contributing-to-an-existing-one">Octopress: Setting up a Blog and Contributing to an Existing One</a></p>
  </li>
  <li>
    <p><a href="https://github.com/echen/echen.github.com/blob/source/_config.yml">echen’s _config.yml</a></p>
  </li>
</ul>

<p>还是想写点技术文章。</p>

]]></content>
  </entry>
  
</feed>
