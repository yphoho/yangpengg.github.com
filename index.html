
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>yangpeng's Blog</title>
  <meta name="author" content="yp">

  
  <meta name="description" content="问题 简单来说，有一堆实数数据，数据的格式如下： x_1, x_2, x_3, \cdots, x_n, y 所有的这些数据称为训练集，其中$x$称为feature，$y$称为target。 现在又来了一个数据： x_1, x_2, x_3, \cdots, x_n 现在需要做的是根据这些$x$ &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yangpengg.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="yangpeng's Blog" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">yangpeng's Blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:yangpengg.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/16/linear-regression-and-the-theory/">Linear Regression and the Theory</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-16T22:30:00+08:00" pubdate data-updated="true">Dec 16<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1 id="section">问题</h1>

<p>简单来说，有一堆实数数据，数据的格式如下：</p>

<script type="math/tex; mode=display">
x_1, x_2, x_3, \cdots, x_n, y
</script>

<p>所有的这些数据称为训练集，其中$x$称为feature，$y$称为target。</p>

<p>现在又来了一个数据：</p>

<script type="math/tex; mode=display">
x_1, x_2, x_3, \cdots, x_n
</script>

<p>现在需要做的是根据这些$x$的值，推测出$y$的值。</p>

<p>对这个问题更详细的描述可以看<a href="http://v.163.com/movie/2008/1/B/O/M6SGF6VB4_M6SGHJ9BO.html">Stanford机器学习公开课</a>中相关描述。</p>

<h1 id="section-1">解决方法</h1>

<h2 id="overdetermined-equations">Overdetermined Equations</h2>

<p>假设$y$是$x$的线性函数（顺便说一句lr中的linear是对于$\theta$而言的，并非针对$x$），表达为公式为：</p>

<script type="math/tex; mode=display">
y = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n
</script>

<p>其中$x_0$为截距(intercept term)，其值恒为1。</p>

<p>最容易想到的方法，可以把所有训练集的数据代入这个公式，得到方程组：</p>

<script type="math/tex; mode=display">
\begin{eqnarray} 
\left\{ 
\begin{array}{lll}
y^{(1)} = \theta_0x_0^{(1)} + \theta_1x_1^{(1)} + \theta_2x_2^{(1)} + \cdots + \theta_nx_n^{(1)} \\
y^{(2)} = \theta_0x_0^{(2)} + \theta_1x_1^{(2)} + \theta_2x_2^{(2)} + \cdots + \theta_nx_n^{(2)} \\
\vdots \\
y^{(m)} = \theta_0x_0^{(m)} + \theta_1x_1^{(m)} + \theta_2x_2^{(m)} + \cdots + \theta_nx_n^{(m)}
\end{array} 
\right. 
\end{eqnarray}
</script>

<p>这个方程组有m个方程，n+1个未知数，实际问题中通常是训练集的个数大于feature个数，也就是说m &gt; n+1，这种情况下的方程组称为<a href="http://en.wikipedia.org/wiki/Overdetermined_system">超定方程组</a>，是不能直接求解的。当然可以像当年欧拉和拉普拉斯最初解决天文计算问题一样(<a href="http://www.52nlp.cn/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%E4%BA%8C">here</a>)，把m个方程组分成n+1组，然后每一组合并成一个方程，得到n+1个方程后再求解。不过问题是怎么分成n+1组，这个很是adhoc的。</p>

<h2 id="cost-function">Cost Function</h2>

<p>机器学习上解决这个问题的方法是定义一个损失函数：</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2
</script>

<p>然后选择适当的$\theta$，使得$J(\theta)$最小。</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>这个最小化的算法在机器学习中称为梯度下降：</p>

<ul>
  <li>随机初始化一组$\theta$值；</li>
  <li>朝着减少cost function的方向，不断更新$\theta$值，直到收敛。更新公式为：</li>
</ul>

<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial \theta_j}
</script>

<p>其中$\alpha$为学习速率(learning rate)。</p>

<h2 id="gradient-descent-1">Gradient Descent推导</h2>

<p>假设训练集中只有一个数据，$\frac{\partial J(\theta)}{\partial \theta_j}$计算如下：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align*}
\frac{\partial J(\theta)}{\partial \theta_j} &= \frac{\partial{(\frac{1}{2}(h_\theta(x) - y)^2)}}{\partial \theta_j} \\
&= 2 * \frac{1}{2}(h_\theta(x) - y) * \frac{\partial{(h_\theta(x) - y)}}{\partial \theta_j} \\
&= (h_\theta(x) - y) * \frac{\partial(h_\theta(x) - y)}{\partial \theta_j} \\
&= (h_\theta(x) - y) * \frac{\partial(\sum_{i=0}^n\theta_ix_i - y)}{\partial \theta_j} \\
&= (h_\theta(x) - y)x_j
\end{align*}
 %]]></script>

<p>代入更新公式：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align*}
\theta_j &= \theta_j - \alpha(h_\theta(x) - y)x_j \\
&= \theta_j + \alpha(y - h_\theta(x))x_j
\end{align*}
 %]]></script>

<p>对于有m个数据集的情况可以得到如下公式：</p>

<script type="math/tex; mode=display">
\begin{align*}
\theta_j := \theta_j + \alpha\sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}
\end{align*}
</script>

<h2 id="gradient-descent-2">Gradient Descent直观解释</h2>

<p>$J(\theta)$是一个关于$\theta$的多元函数，高等数学的知识说，$J(\theta)$在点<script type="math/tex">P(\theta_0, \theta_1, \cdots, \theta_n)</script>延梯度方向上升最快。现在要最小化 $J(\theta)$，为了让$J(\theta)$尽快收敛，就在更新$\theta$时减去其在P点的梯度。</p>

<p>在最终推导出的更新公式中，可以得出以下直观结论：如果遇到一个数据使得$(y - h_\theta(x))$比较小，这时候$\theta$的更新也会很小，这也符合直观感觉。当一个数据使得差值比较大时，$\theta$的更新也会比较大。</p>

<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>

<p>以上的讨论的算法叫batch gradient descent，batch指的是，每次更新$\theta$的时候都需要所有的数据集。这个算法有两个缺陷：</p>

<ul>
  <li>数据集很大时，训练过程计算量太大；</li>
  <li>需要得到所有的数据才能开始训练；</li>
</ul>

<p>比如一个场景下，我们训练了一个lr模型，应用于线上环境，当这个模型跑在线上的时候我们会收集更多的数据。但是上面两个问题使得我们不能及时更新模型，而这正是随机梯度下降要解决的问题。</p>

<p>在之前的推导过程中已经给出了sgd的更新公式，只是没有指出，现正式提出sgd的更新公式：</p>

<p>loop for every (x, y) in training set until convergence:</p>

<script type="math/tex; mode=display">
\theta_j := \theta_j + \alpha(y - h_\theta(x))x_j
</script>

<p>与bgd唯一的区别是，无论数据集有多少，每次迭代都只用一个数据。这样当有新的数据时，直接通过上式更新$\theta$，这就是所谓的online learning。又因为每次更新都只用到一个数据，所以可以显著减少计算量。</p>

<h1 id="sgdpython">sgd的Python实现</h1>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#!/usr/bin/env python
</span><span class="line"># -*- coding: utf-8 -*-
</span><span class="line">
</span><span class="line">import re
</span><span class="line">import numpy as np
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">def load_data(filename):                                                
</span><span class="line">    feature = []                                                        
</span><span class="line">    target = []                                                        
</span><span class="line">    f = open(filename, 'rb')                                            
</span><span class="line">    for line in f.readlines():                                          
</span><span class="line">        sample = re.split(' +', line.strip())                          
</span><span class="line">        feature.append([1, ] + sample[0:-1])                             
</span><span class="line">        target.append(sample[-1])                                      
</span><span class="line">                                                                       
</span><span class="line">    return np.array(feature, np.float), np.array(target, np.float)       
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">def sgd(feature, target, iter=200, step=0.001):                       
</span><span class="line">    theta = np.zeros(feature.shape[1])                                  
</span><span class="line">    # theta = np.ones(feature.shape[1])                                
</span><span class="line">                                                                       
</span><span class="line">    for it in range(iter):                                              
</span><span class="line">        for i in range(feature.shape[0]):                             
</span><span class="line">            error = target[i] - sum(theta * feature[i])                
</span><span class="line">            theta = theta + step * error * feature[i]                  
</span><span class="line">                                                                       
</span><span class="line">        predict = [sum(theta * sample) for sample in feature]            
</span><span class="line">        mse = sum((target - predict) ** 2) / feature.shape[0]            
</span><span class="line">        print it, 'mse:', mse
</span><span class="line">                                                                       
</span><span class="line">    return theta                                                        
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">def normalize(feature):                                                
</span><span class="line">    mu = feature.mean(0)                                                
</span><span class="line">    std = feature.std(0)                                                
</span><span class="line">                                                                       
</span><span class="line">    for j in range(1, feature.shape[1]):                                
</span><span class="line">        feature[:, j] = (feature[:, j] - mu[j]) / std[j]                 
</span><span class="line">                                                                       
</span><span class="line">    return feature, mu, std                                            
</span><span class="line">                                                                       
</span><span class="line">                                                                       
</span><span class="line">if __name__ == '__main__':                                              
</span><span class="line">    datafile = 'housing_data'
</span><span class="line">    x, y = load_data(datafile)
</span><span class="line">    x, mu, std = normalize(x)
</span><span class="line">    theta = sgd(x, y)
</span><span class="line">    print theta</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>代码中使用的数据集可以从<a href="http://amachinelearningtutorial.googlecode.com/files/housing_data">这里</a>下载，描述在<a href="http://amachinelearningtutorial.googlecode.com/files/housing_description">这里</a>。</p>

<p>代码中normalize函数用于对feature进行归一化处理，可以尝试一下去掉normalize过程，对于这个数据集会得出很出乎意料的结果。</p>

<h1 id="section-2">概率解释</h1>

<p>在以上的讨论中，得出$y$与$x$的关系是线性假设，使用梯度下降也可以从高数中得到依据，唯有损失函数好像是拍脑袋想出来的。有那么多的函数可以用，为什么单选择了一个二次式做为损失函数。其实这里选择二次函数是有其理论基础的。</p>

<p>$y$与$x$满足以下公式：</p>

<script type="math/tex; mode=display">
y^{(i)} = \theta^{T}x^{(i)} + \varepsilon^{(i)}
</script>

<p>其中$\varepsilon^{(i)}$称为误差，可能由两个原因产生：</p>

<ul>
  <li>feature选择的不合适；</li>
  <li>随机噪声；</li>
</ul>

<p>又假设$\varepsilon^{(i)}$独立同分布，且满足均值为0，方差为$\sigma^2$的高斯分布，即：</p>

<script type="math/tex; mode=display">
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\varepsilon^{(i)})^2}{2\sigma^2}}
</script>

<p>也就是：</p>

<script type="math/tex; mode=display">
p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}}
</script>

<p>以上是一个关于$y$, $X$的公式，可以定义一个似然函数，形式如同上式，但是似然函数是关于$\theta$的公式：</p>

<script type="math/tex; mode=display">
L(\theta) = L(\theta; X,y) = p(y|X; \theta)
</script>

<p>根据之前$\varepsilon^{(i)}$的独立性假设，$L(\theta)$可以记做</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align*}
L(\theta) &= \prod_{i=1}^m{p(y^{(i)} | x^{(i)}; \theta)} \\
&= \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}}
\end{align*}
 %]]></script>

<p>现在已经观察到了很多数据($x$, $y$)，那么什么样的模型才能让这些数据出现的可能性最大。这就是最大似然估计的出发点，也就是求解$\theta$以最大化这些数据出现的概率，即最大化似然函数$L(\theta)$。</p>

<p>关于最大似然估计方法更多解释可以看<a href="http://www.cchere.com/article/1522559">这里</a>。</p>

<p>当然更多时候最大化的是$logL(\theta)$，而不是直接最大化$L(\theta)$，因为log函数单调递增函数，所以这个转化不会影响$\theta$的最终取值。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align*}
l(\theta) &= logL(\theta) \\
&= log\prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}} \\
&= \sum_{i=1}^m log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}} \\
&= mlog\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
\end{align*}
 %]]></script>

<p>因此最大化$l(\theta)$也就是最小化：</p>

<script type="math/tex; mode=display">
\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
</script>

<p>也就是之前出现的$J(\theta)$。</p>

<p>至此，我们从概率和最大似然估计的角度解释了$J(\theta)$为什么选择这个二次式是合理的。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/15/mathjax-test/">MathJax Test</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-15T23:12:00+08:00" pubdate data-updated="true">Dec 15<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1 id="instruction">Instruction</h1>

<p><a href="http://www.idryman.org/blog/2012/03/10/writing-math-equations-on-octopress/">Here</a> is a good instructions to set the MathJax for Octopress.</p>

<p>There are two things to point:</p>

<ul>
  <li>‘gem install kramdown’ is not needed, cause of the installed when setup Octopress;</li>
  <li>I have not found something useful about ‘change Gemfile to kramdown’;</li>
</ul>

<h1 id="latex-test">Latex Test</h1>

<h2 id="gaussian-function">Gaussian Function</h2>

<script type="math/tex; mode=display">
f(x; \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
</script>

<h2 id="equation-used-in-gradient-descent">Equation Used in Gradient Descent</h2>

<script type="math/tex; mode=display">
h_\theta(x) = \sum_{i=0}^n\theta_ix_i = \Theta^Tx
</script>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^i) - y^i)^2
</script>

<script type="math/tex; mode=display">
\theta_j := \theta_j + \alpha\sum_{i=1}^m(y^i - h_\theta(x^i))x_j^i
</script>

<h2 id="a-long-equation">A Long Equation</h2>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align*}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
 %]]></script>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/13/decode-and-encode-in-python/">Decode and Encode in Python</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-13T15:09:00+08:00" pubdate data-updated="true">Dec 13<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文主要针对2.7.x版本，对3.x版本可能不适用。</p>

<h1 id="decode-encode">什么是Decode, Encode</h1>

<p>Python内部使用Unicode对所有的字符编码。对于Python而言，Decode指的是将其它编码转化为Unicode，Encode指把Unicode转化为其它编码。</p>

<p>本文只针对utf-8与Unicode的转化，gbk, gb2312就不添乱了。不过可以简单说一句，Python识别源文件的编码主要根据两个内容：</p>

<ul>
  <li>源码中的coding，如：</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line"># -*- coding: utf-8 -*-</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>源码文件的实际编码</li>
</ul>

<h1 id="decode-encode-unicode">decode, encode, unicode函数</h1>

<p>在Python console输入以下内容：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ustring = u'我们'
</span><span class="line">ustring                       # u'\u6211\u4eec'
</span><span class="line">ustring.encode('utf-8')       # '\xe6\x88\x91\xe4\xbb\xac'
</span><span class="line">
</span><span class="line">string = '我们'
</span><span class="line">string                        # '\xe6\x88\x91\xe4\xbb\xac'
</span><span class="line">string.decode('utf-8')        # u'\u6211\u4eec'
</span><span class="line">unicode(string, 'utf-8')      # u'\u6211\u4eec'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>以上的内容是Mac下的结果，在Linux应该也没有问题，但不保证在Windows下也会产生相同的结果。原因还是以上两条，而Mac和Linux的shell编码是utf-8的，Windows就不清楚了。</p>

<p>从以上的代码可以得出：</p>

<ul>
  <li>
    <p>unicode和decode的作用是一样的，都是把其它编码转化成Unicode；</p>
  </li>
  <li>
    <p>字符前加’u’表示Unicode编码，不加任何东西则是默认编码；</p>
  </li>
  <li>
    <p>encode和decode确实是按照之前提到的那样工作；</p>
  </li>
</ul>

<h1 id="section">读写文件时的编码转化</h1>

<p>继续输入以下代码：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">open('unicode.txt', 'wb').write(ustring)                   # UnicodeDecodeError
</span><span class="line">open('utf-8.txt', 'wb').write(ustring.encode('utf-8'))
</span><span class="line">open('utf-8.txt', 'rb').read()                             # '\xe6\x88\x91\xe4\xbb\xac'
</span><span class="line">open('utf-8.txt', 'rb').read().decode('utf-8')             # u'\u6211\u4eec'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>第一条语句是报错的，也就是说Python的内部编码字符串是不能直接写到文件的，应该先encode到一种编码之后再写入到文件。</p>

<p>read调用读取的是原始内容，不经过任何编码，所以需要在程序中自己处理编码问题。比如要把gbk转化成utf-8，则这样调用：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ustring = open('gbk.txt', 'rb').read().decode('gbk')
</span><span class="line">open('utf-8.txt', 'wb').write(ustring.encode('utf-8'))</span></code></pre></td></tr></table></div></figure></notextile></div>

<h1 id="json">读写json时的编码转化</h1>

<h2 id="jsondump">json.dump</h2>

<p>输入以下代码：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">import json
</span><span class="line">
</span><span class="line">address = [{'name': '杨xx', 'country': '中国'}, {'name': '卢xx', 'country': '美国'}]
</span><span class="line">json.dump(address, open('address.json', 'wb'), indent=2)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>然后打开address.json文件，看到的内容如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">[
</span><span class="line">  {
</span><span class="line">    "country": "\u4e2d\u56fd", 
</span><span class="line">    "name": "\u6768xx"
</span><span class="line">  }, 
</span><span class="line">  {
</span><span class="line">    "country": "\u7f8e\u56fd", 
</span><span class="line">    "name": "\u5362xx"
</span><span class="line">  }
</span><span class="line">]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>之所以看到这样的结果，是因为json模块内部对address转化成字串的时候，把字串从其它编码解码为Unicode（其实json.dump还有一个参数encoding，默认为’utf-8’，这里没有给出），然后又对Unicode进行了转义（这里write没有报UnicodeDecodeError，是因为json内部把Unicode二进制转化成了’\u4e2d’这样的串，这就是所谓的转义），最后write转义后的字符。</p>

<p>如果这种情况下想要等到正常的中文串，可以这样做：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">json.dump(address, open('address2.json', 'wb'), indent=2, ensure_ascii=False)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>设置ensure_ascii=False禁止json进行解码和转义，又因为address的编码就是’utf-8’的，所以write的时候不会报错。</p>

<p>但是如果遇到这种情况：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">address = [{u'name': u'杨xx', u'country': u'中国'},
</span><span class="line">           {u'name': u'卢xx', u'country': u'美国'}]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>直接调用：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">json.dump(address, open('address.json', 'wb'), indent=2)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>没有悬念，会得到如上address.json的结果。需要指出的是这次json在内部判断出字符是Unicode编码，所以省略了解编的步骤，只进行转义。</p>

<p>如果想得到中文，按照以上的经验应该输入:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">json.dump(address, open('address2.json', 'wb'), indent=2, ensure_ascii=False)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>好吧，出错了，又是UnicodeEncodeError。简单分析一下可以得到结论，ensure_ascii=False保证不会进行解码（当然这里不需要解码）和转义，这样json输出给write的就直接是Unicode字符，之前已经提到过了，write是不能直接写Unicode的。</p>

<p>那么如何让json输出正常的中文呢？一种比较容易想到的就是把整个address的字符全部用utf-8编码之后再像写address2.json那样输出。但是这种方法代价比较大，另一种比较好的方式是：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">jsonunicodestr = json.dumps(address, indent=2, ensure_ascii=False)
</span><span class="line">open('address2.json', 'wb').write(jsonunicodestr.encode('utf-8'))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>第一句把address转化成一个字符串，而且不进行转义，所以整个jsonunicodestr是Unicode字串。然后把字串编码成utf-8写入文件。</p>

<p>当然也可以合二为一：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">import codecs
</span><span class="line">
</span><span class="line">json.dump(address, codecs.open('address2.json', 'wb', 'utf-8'), indent=2, ensure_ascii=False)</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="jsonload">json.load</h2>

<p>无论是从address.json还是address2.json，调用json.load的时候得到的都是Unicode编码的字符。</p>

<h1 id="python">关于Python编码处理几点想法</h1>
<ul>
  <li>任何时候使用utf-8；</li>
  <li>怎么进，怎么出，尽量不涉及编码问题；</li>
  <li>json模块是个意外，经过json的所有字符都会解码成Unicode；</li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/11/02/view-transition-follow-finger/">View Transition follow Finger</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-11-02T11:11:00+08:00" pubdate data-updated="true">Nov 2<span>nd</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I use <a href="http://getprismatic.com/">Prismatic</a> as my main reading tool on iphone. I find the view transition of the app is very intresting(you can have a try <a href="https://itunes.apple.com/us/app/prismatic-always-interesting/id551206444?mt=8">here</a>), that i want to know how it does. By google, i find a <a href="http://iappexperience.com/post/23551184719/chromeless">blog</a> and you can get the <a href="https://github.com/dyang/DYNavigationController">source code</a>. With the demo you can swipe the ui to transition, but can’t let the transition follows your finger. So i decide to write a demo myself.</p>

<p>I put my code on <a href="https://github.com/hermitinhistory/FollowFinger">github</a>, and you can get the most explanation from the blog above.</p>

<p>Finally, big thanks to the author of the blog.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/05/minhash-and-lsh/">Minhash and LSH</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-05T09:05:00+08:00" pubdate data-updated="true">Sep 5<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文主要参考<a href="http://i.stanford.edu/~ullman/mmds.html">Mining of Massive Datasets</a>中第三章相关内容。</p>

<h1 id="jaccard-similarity">Jaccard Similarity</h1>

<p>在推荐系统经常要做相似度计算，其中比较简单的一种是<a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard Similarity</a>。Jaccard Similarity描述如下：<br />
设A,B为两个特征向量，比如为两个user投过票的电影，则  </p>

<script type="math/tex; mode=display">
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
</script>

<p>J(A,B)的值介于0-1之间，数值越大，相似度越高。</p>

<p>Jaccard相似度计算很简单，但是当特征向量维数很高的时候(比如文档的特征向量经常会上万维)，这个计算也会很耗时，这时通用的方法是降维，Minhash也可以算作针对Jaccard相似度的降维方法。</p>

<h1 id="minhash">Minhash</h1>

<p>Minhash思想是用一个维度比较小的<em>signatures</em>代替原来维度很大的特征向量，而且可以通过计算这个signatures向量Jaccard相似度去估计原来特征向量的Jaccard相似度。</p>

<h2 id="section">特征向量的矩阵表示</h2>

<p>如S1 = {a, d}, S2 = {c}, S3 = {b, d, e}, S4 = {a, c, d}，则表示为矩阵：</p>

<p><img src="/images/blogpng/matrix-representation.png" alt="Matrix Representation" /></p>

<p>以电商为例，这个矩阵可以解释为：用户S1购买过a,d两个商品，S2购买过c，S3购买过b,d,e，…</p>

<h2 id="minhash-1">Minhash计算原理</h2>

<p>为了推荐物品给S1，思路可以是计算出和S1相似的用户，查看这个(些)用户的购买记录，推荐其中S1没有购买过的商品。可以使用Jaccard相似度计算S1的相似用户，但是现在用户的特征向量是5维的，是不是可以使用更少的维度就可以得到Jaccard相似度。</p>

<p>取Element的一个全排列，以beadc为例，这个排列定义了一个hash函数，这个函数的矩阵可以表示：</p>

<p><img src="/images/blogpng/permutation.png" alt="a permutation" /></p>

<p>根据这个hash函数，每个特征向量的minhash值定义为：第一个value不为0的Element的值。</p>

<p>从矩阵中可以的得到：<br />
h(S1) = a, h(S2) = c, h(S3) = b, h(S4) = a</p>

<p>可以取另一个Element的不同的全排列，再进行一次以上的步骤，可以得到一组不同的minhash值。将所有的minhash值组合成矩阵，则新生成的矩阵可以作为新的特征向量。如果只取两个排列的话，则原来的5维特征向量降到了2维。</p>

<h2 id="minhash-2">计算Minhash</h2>

<p>实际应用中，如果特征维度很高的话，产生一次全排列是很费时的，所以通常并不会通过产生全排列计算minhash值。</p>

<p>如果特征向量为k维，则可以取n个hash函数，将0,1,…,k-1重新映射到0,1,…,k-1。类比于全排列，这些hash函数的意义相当于将第r维的特征放到了h(r)维，相当于产生了n个全排列。当然hash函数不可避免会有冲突，但是只要k足够大(如果不够大的话，就不需要minhash了)，而冲突不太多的时候，这个影响可以忽略。</p>

<p>如果以h1,h2,…,hn表示n个随机hash函数，r表示原矩阵的行数，SIG(i, c)表示为新生成矩阵第i个hash函数(也就是第i行)，第c列的元素。首先设所有SIG中的元素为MAXINT，然后对每一行r作如想处理：</p>

<p><img src="/images/blogpng/algo-minhash.png" alt="algo minhash" /></p>

<p>Minhash的python实现：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#!/usr/bin/env python
</span><span class="line"># -*- coding: utf-8 -*-
</span><span class="line">
</span><span class="line">
</span><span class="line">def minhash(data, hashfuncs):
</span><span class="line">    '''
</span><span class="line">        see mining-of-massive-datasets.pdf ch3 minhash for detail
</span><span class="line">    '''
</span><span class="line">
</span><span class="line">    # DEBUG = True
</span><span class="line">    DEBUG = False
</span><span class="line">
</span><span class="line">    rows, cols, sigrows = len(data), len(data[0]), len(hashfuncs)
</span><span class="line">
</span><span class="line">    # fucking the shadow copy
</span><span class="line">    # sigmatrix = [[1000000] * cols] * sigrows
</span><span class="line">    sigmatrix = []
</span><span class="line">    for i in range(sigrows):
</span><span class="line">        sigmatrix.append([10000000] * cols)
</span><span class="line">
</span><span class="line">    for r in range(rows):
</span><span class="line">        hashvalue = map(lambda x: x(r), hashfuncs)
</span><span class="line">        if DEBUG: print hashvalue
</span><span class="line">        for c in range(cols):
</span><span class="line">            if DEBUG: print '-' * 2, r, c
</span><span class="line">            if data[r][c] == 0:
</span><span class="line">                continue
</span><span class="line">            for i in range(sigrows):
</span><span class="line">                if DEBUG: print '-' * 4, i, sigmatrix[i][c], hashvalue[i]
</span><span class="line">                if sigmatrix[i][c] &gt; hashvalue[i]:
</span><span class="line">                    sigmatrix[i][c] = hashvalue[i]
</span><span class="line">                if DEBUG: print '-' * 4, sigmatrix
</span><span class="line">
</span><span class="line">        if DEBUG:
</span><span class="line">            for xxxxxxx in sigmatrix:
</span><span class="line">                print xxxxxxx
</span><span class="line">            print '=' * 30
</span><span class="line">
</span><span class="line">    return sigmatrix
</span><span class="line">
</span><span class="line">
</span><span class="line">if __name__ == '__main__':
</span><span class="line">    def hash1(x):
</span><span class="line">        return (x + 1) % 5
</span><span class="line">
</span><span class="line">    def hash2(x):
</span><span class="line">        return (3 * x + 1) % 5
</span><span class="line">
</span><span class="line">    data = [[1, 0, 0, 1],
</span><span class="line">            [0, 0, 1, 0],
</span><span class="line">            [0, 1, 0, 1],
</span><span class="line">            [1, 0, 1, 1],
</span><span class="line">            [0, 0, 1, 0]]
</span><span class="line">
</span><span class="line">    print minhash(data, [hash1, hash2])</span></code></pre></td></tr></table></div></figure></notextile></div>

<h1 id="locality-sensitive-hashing">Locality-Sensitive Hashing</h1>

<p>在推荐系统中，通常需要的是与S1 n个最相似的，而不是所有的。<a href="http://en.wikipedia.org/wiki/Locality_sensitive_hashing">Locality-Sensitive Hashing</a>的提出就是为了过滤明显不相似的，以此来减少计算。</p>

<p>假设向量的维度为n，将n维分为b份，称为bands，每份为n/b维。同时存在b个hash函数，每个bands对应一个hash函数，每个hash函数接受n/b个参数，产生一个数值，把相同数值的元素分为一组。</p>

<p>对每个binds中的n/b维特征向量进行hash计算，比如对于{S1, S2, S3, S4}，3个binds而言，第一个binds可能产生的数据为{S1, S2}, {S3}, {S4}，第二个binds为{S1}, {S2, S4}, {S3}，第三个binds为{S1, S3}, {S2, S4}。则为了计算得到和S1最相似的n个，则只需要计算J(S1, S2), J(S1, S3)，这样达到了减少了计算的目的。</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/08/21/octopress-setup/">Octopress Setup</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-08-21T16:02:00+08:00" pubdate data-updated="true">Aug 21<span>st</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本来没有写过blog，4月份弄了个vps，顺便在上边搭了个wordpress，原打算是要写些技术类的文章，但是一直也没有这个习惯，所以到现在那上边也只有一篇。现在想把blog放到github上，正好也改用Octopress。</p>

<p>Octopress部署起来也没有什么麻烦，只是有一点要注意，想在github上搭User Page，Repository name必须是username.github.com，比如我的用户名是yangpengg，repository就只能是yangpengg.github.com，其它的都不能正常部署。</p>

<p>主要参考以下内容：</p>

<ul>
  <li>
    <p><a href="http://octopress.org/docs/">Octopress Documentation</a></p>
  </li>
  <li>
    <p><a href="http://code.dblock.org/octopress-setting-up-a-blog-and-contributing-to-an-existing-one">Octopress: Setting up a Blog and Contributing to an Existing One</a></p>
  </li>
  <li>
    <p><a href="https://github.com/echen/echen.github.com/blob/source/_config.yml">echen’s _config.yml</a></p>
  </li>
</ul>

<p>还是想写点技术文章。</p>

</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Yang Peng</h1>
  
  <p>I like machine learning, recommender system and nlp.</p>

  <p>Email: hermitinhistory[at]gmail.com<br/>
  Twitter: <a href="https://twitter.com/#!/yangpengg">@yangpengg</a><br/>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/12/16/linear-regression-and-the-theory/">Linear Regression and the Theory</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/15/mathjax-test/">MathJax Test</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/13/decode-and-encode-in-python/">Decode and Encode in Python</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/11/02/view-transition-follow-finger/">View Transition follow Finger</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/09/05/minhash-and-lsh/">Minhash and LSH</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/08/21/octopress-setup/">Octopress Setup</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("yangpengg", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/yangpengg" class="twitter-follow-button" data-show-count="true">Follow @yangpengg</a>
  
</section>


  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - yp -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>


<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
